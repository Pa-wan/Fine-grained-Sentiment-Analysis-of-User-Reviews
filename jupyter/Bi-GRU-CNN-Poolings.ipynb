{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "np.random.seed(32)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input,  CuDNNLSTM, Embedding, Dropout, Activation, Conv1D, CuDNNGRU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/home/xq/data/embed/wiki.zh.vec'\n",
    "train = pd.read_csv(\"../inputs/train.csv\")\n",
    "test = pd.read_csv(\"../inputs/testa.csv\")\n",
    "val = pd.read_csv(\"../inputs/vali.csv\")\n",
    "X_train = train[\"content\"].fillna(\"无\").str.lower()\n",
    "x_val = val[\"content\"].fillna(\"无\").str.lower()\n",
    "X_test = test[\"content\"].fillna(\"无\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train.columns.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from typing import List\n",
    "def get_y(df:pd.DataFrame, cols:List[str]=columns) -> List[np.array]:\n",
    "    y_list = []\n",
    "    for col in cols:\n",
    "        y = df[col].values + 2\n",
    "        y_ = to_categorical(y, num_classes=4)\n",
    "        y_list.append(y_)\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_y(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = get_y(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=50000\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from  tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from  tensorflow.keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "\n",
    "file_path = \"../ckpt/best_model_bigru_cnn_2.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n",
    "reduce_plateau = ReduceLROnPlateau(factor=0.5, patience=2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_0(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, classes=20):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(128, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    \n",
    "    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(128, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "    ys = []\n",
    "    for i in range(classes):\n",
    "        y = Dense(4, activation = \"softmax\")(x)\n",
    "        ys.append(y)\n",
    "    model = Model(inputs = inp, outputs = ys)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(x_train, y_train, batch_size = 256, epochs = 100,validation_split=0.05 , \n",
    "                        verbose = 1, callbacks = [check_point, early_stop, reduce_plateau])\n",
    "    model = load_model(file_path)\n",
    "    return model\n",
    "# 从卷基层开始不共享\n",
    "def build_model_2(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.4, classes=20):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    ys = []\n",
    "    for i in range(classes):\n",
    "        x2 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "        y2 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x2)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "        avg_pool2 = GlobalAveragePooling1D()(y2)\n",
    "        max_pool2 = GlobalMaxPooling1D()(y2)\n",
    "\n",
    "\n",
    "        x3 = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "        y3 = Dense(4, activation = \"softmax\")(x3)\n",
    "        ys.append(y3)\n",
    "    model = Model(inputs = inp, outputs = ys)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(x_train, y_train, batch_size = 256, epochs = 100,validation_split=0.05 , \n",
    "                        verbose = 1, callbacks = [check_point, early_stop, reduce_plateau])\n",
    "    model = load_model(file_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model_2(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tok.texts_to_sequences(x_val)\n",
    "x_val =pad_sequences(x_val,maxlen=maxlen)\n",
    "test_pred = model.predict(x_test)\n",
    "val_pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_list = []\n",
    "for pred, true in zip(val_pred, y_val):\n",
    "    F1 = f1_score(np.argmax(pred, axis=1), np.argmax(true, axis=1),average='macro')\n",
    "    print(F1)\n",
    "    f1_list.append(F1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_name, header=0, encoding=\"utf-8\"):\n",
    "\n",
    "    data_df = pd.read_csv(file_name, header=header, encoding=encoding)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data_from_csv(\"../inputs/sentiment_analysis_testa.csv\")\n",
    "for pred, column in zip(test_pred, columns):\n",
    "    test[column] = np.argmax(pred, axis=1) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"../output/bigru-cnn-pooling3.csv\", encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
