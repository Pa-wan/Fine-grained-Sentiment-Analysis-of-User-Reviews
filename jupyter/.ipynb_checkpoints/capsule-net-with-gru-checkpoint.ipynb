{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xq/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "np.random.seed(32)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input,  CuDNNLSTM, Embedding, Dropout, Activation, Conv1D, CuDNNGRU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import Callback     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train.columns.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from typing import List\n",
    "def get_y(df:pd.DataFrame, cols:List[str]=columns) -> List[np.array]:\n",
    "    y_list = []\n",
    "    for col in cols:\n",
    "        y = df[col].values + 2\n",
    "        y_ = to_categorical(y, num_classes=4)\n",
    "        y_list.append(y_)\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_y(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = get_y(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=50000\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from  tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from  tensorflow.keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "\n",
    "file_path = \"../ckpt/best_model_bigru_cnn_2.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n",
    "reduce_plateau = ReduceLROnPlateau(factor=0.5, patience=2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_0(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, classes=20):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(128, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    \n",
    "    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(128, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "    ys = []\n",
    "    for i in range(classes):\n",
    "        y = Dense(4, activation = \"softmax\")(x)\n",
    "        ys.append(y)\n",
    "    model = Model(inputs = inp, outputs = ys)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(x_train, y_train, batch_size = 256, epochs = 100,validation_split=0.05 , \n",
    "                        verbose = 1, callbacks = [check_point, early_stop, reduce_plateau])\n",
    "    model = load_model(file_path)\n",
    "    return model\n",
    "# 从卷基层开始不共享\n",
    "def build_model_2(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.4, classes=20):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    y = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    ys = []\n",
    "    for i in range(classes):\n",
    "        x2 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "        y2 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x2)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "        avg_pool2 = GlobalAveragePooling1D()(y2)\n",
    "        max_pool2 = GlobalMaxPooling1D()(y2)\n",
    "\n",
    "\n",
    "        x3 = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "        y3 = Dense(4, activation = \"softmax\")(x3)\n",
    "        ys.append(y3)\n",
    "    model = Model(inputs = inp, outputs = ys)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(x_train, y_train, batch_size = 256, epochs = 100,validation_split=0.05 , \n",
    "                        verbose = 1, callbacks = [check_point, early_stop, reduce_plateau])\n",
    "    model = load_model(file_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/100\n",
      "99750/99750 [==============================] - 83s 834us/step - loss: 15.7775 - dense_loss: 0.5429 - dense_1_loss: 0.5378 - dense_2_loss: 0.7078 - dense_3_loss: 0.4940 - dense_4_loss: 1.1154 - dense_5_loss: 0.2911 - dense_6_loss: 0.5895 - dense_7_loss: 1.1954 - dense_8_loss: 0.7200 - dense_9_loss: 0.9834 - dense_10_loss: 0.8760 - dense_11_loss: 0.7405 - dense_12_loss: 0.9347 - dense_13_loss: 0.8240 - dense_14_loss: 1.1246 - dense_15_loss: 0.9623 - dense_16_loss: 0.7963 - dense_17_loss: 0.5971 - dense_18_loss: 0.8775 - dense_19_loss: 0.8672 - dense_acc: 0.8101 - dense_1_acc: 0.7995 - dense_2_acc: 0.7661 - dense_3_acc: 0.8814 - dense_4_acc: 0.5059 - dense_5_acc: 0.9348 - dense_6_acc: 0.8430 - dense_7_acc: 0.5098 - dense_8_acc: 0.7639 - dense_9_acc: 0.6097 - dense_10_acc: 0.6346 - dense_11_acc: 0.7325 - dense_12_acc: 0.6550 - dense_13_acc: 0.6849 - dense_14_acc: 0.5384 - dense_15_acc: 0.5292 - dense_16_acc: 0.7214 - dense_17_acc: 0.8091 - dense_18_acc: 0.6676 - dense_19_acc: 0.6282 - val_loss: 13.8436 - val_dense_loss: 0.4367 - val_dense_1_loss: 0.4901 - val_dense_2_loss: 0.6645 - val_dense_3_loss: 0.4628 - val_dense_4_loss: 0.8535 - val_dense_5_loss: 0.1963 - val_dense_6_loss: 0.5274 - val_dense_7_loss: 1.0145 - val_dense_8_loss: 0.7018 - val_dense_9_loss: 0.9306 - val_dense_10_loss: 0.7004 - val_dense_11_loss: 0.6497 - val_dense_12_loss: 0.8682 - val_dense_13_loss: 0.6544 - val_dense_14_loss: 1.1111 - val_dense_15_loss: 0.8481 - val_dense_16_loss: 0.7740 - val_dense_17_loss: 0.4762 - val_dense_18_loss: 0.7701 - val_dense_19_loss: 0.7133 - val_dense_acc: 0.8630 - val_dense_1_acc: 0.8217 - val_dense_2_acc: 0.7697 - val_dense_3_acc: 0.8882 - val_dense_4_acc: 0.6777 - val_dense_5_acc: 0.9499 - val_dense_6_acc: 0.8411 - val_dense_7_acc: 0.5764 - val_dense_8_acc: 0.7587 - val_dense_9_acc: 0.6244 - val_dense_10_acc: 0.7328 - val_dense_11_acc: 0.7659 - val_dense_12_acc: 0.6830 - val_dense_13_acc: 0.7642 - val_dense_14_acc: 0.5301 - val_dense_15_acc: 0.6023 - val_dense_16_acc: 0.7240 - val_dense_17_acc: 0.8347 - val_dense_18_acc: 0.6876 - val_dense_19_acc: 0.7330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 13.84357, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 2/100\n",
      "99750/99750 [==============================] - 75s 750us/step - loss: 12.8068 - dense_loss: 0.4113 - dense_1_loss: 0.4703 - dense_2_loss: 0.6227 - dense_3_loss: 0.4521 - dense_4_loss: 0.8308 - dense_5_loss: 0.1906 - dense_6_loss: 0.4522 - dense_7_loss: 0.9449 - dense_8_loss: 0.5979 - dense_9_loss: 0.8438 - dense_10_loss: 0.6829 - dense_11_loss: 0.6001 - dense_12_loss: 0.8184 - dense_13_loss: 0.6204 - dense_14_loss: 0.9642 - dense_15_loss: 0.8026 - dense_16_loss: 0.7401 - dense_17_loss: 0.4593 - dense_18_loss: 0.7288 - dense_19_loss: 0.5734 - dense_acc: 0.8642 - dense_1_acc: 0.8271 - dense_2_acc: 0.7865 - dense_3_acc: 0.8832 - dense_4_acc: 0.6872 - dense_5_acc: 0.9529 - dense_6_acc: 0.8696 - dense_7_acc: 0.6300 - dense_8_acc: 0.7994 - dense_9_acc: 0.6619 - dense_10_acc: 0.7391 - dense_11_acc: 0.7907 - dense_12_acc: 0.7056 - dense_13_acc: 0.7818 - dense_14_acc: 0.6344 - dense_15_acc: 0.6305 - dense_16_acc: 0.7344 - dense_17_acc: 0.8416 - dense_18_acc: 0.7087 - dense_19_acc: 0.7903 - val_loss: 11.8650 - val_dense_loss: 0.3997 - val_dense_1_loss: 0.4390 - val_dense_2_loss: 0.5621 - val_dense_3_loss: 0.4009 - val_dense_4_loss: 0.7431 - val_dense_5_loss: 0.1849 - val_dense_6_loss: 0.4280 - val_dense_7_loss: 0.8846 - val_dense_8_loss: 0.5428 - val_dense_9_loss: 0.8391 - val_dense_10_loss: 0.6351 - val_dense_11_loss: 0.5930 - val_dense_12_loss: 0.7912 - val_dense_13_loss: 0.5658 - val_dense_14_loss: 0.8886 - val_dense_15_loss: 0.7228 - val_dense_16_loss: 0.7075 - val_dense_17_loss: 0.4071 - val_dense_18_loss: 0.6461 - val_dense_19_loss: 0.4836 - val_dense_acc: 0.8697 - val_dense_1_acc: 0.8402 - val_dense_2_acc: 0.8126 - val_dense_3_acc: 0.8876 - val_dense_4_acc: 0.7242 - val_dense_5_acc: 0.9556 - val_dense_6_acc: 0.8752 - val_dense_7_acc: 0.6653 - val_dense_8_acc: 0.8232 - val_dense_9_acc: 0.6707 - val_dense_10_acc: 0.7594 - val_dense_11_acc: 0.7901 - val_dense_12_acc: 0.7166 - val_dense_13_acc: 0.7973 - val_dense_14_acc: 0.6672 - val_dense_15_acc: 0.6836 - val_dense_16_acc: 0.7480 - val_dense_17_acc: 0.8684 - val_dense_18_acc: 0.7480 - val_dense_19_acc: 0.8244\n",
      "\n",
      "Epoch 00002: val_loss improved from 13.84357 to 11.86504, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 3/100\n",
      "99750/99750 [==============================] - 73s 731us/step - loss: 11.5587 - dense_loss: 0.3909 - dense_1_loss: 0.4512 - dense_2_loss: 0.5156 - dense_3_loss: 0.4139 - dense_4_loss: 0.7422 - dense_5_loss: 0.1792 - dense_6_loss: 0.4134 - dense_7_loss: 0.8613 - dense_8_loss: 0.5212 - dense_9_loss: 0.7765 - dense_10_loss: 0.6342 - dense_11_loss: 0.5680 - dense_12_loss: 0.7463 - dense_13_loss: 0.5754 - dense_14_loss: 0.8476 - dense_15_loss: 0.7122 - dense_16_loss: 0.6862 - dense_17_loss: 0.4148 - dense_18_loss: 0.6324 - dense_19_loss: 0.4764 - dense_acc: 0.8698 - dense_1_acc: 0.8334 - dense_2_acc: 0.8350 - dense_3_acc: 0.8848 - dense_4_acc: 0.7221 - dense_5_acc: 0.9553 - dense_6_acc: 0.8816 - dense_7_acc: 0.6708 - dense_8_acc: 0.8248 - dense_9_acc: 0.6922 - dense_10_acc: 0.7577 - dense_11_acc: 0.8049 - dense_12_acc: 0.7281 - dense_13_acc: 0.7983 - dense_14_acc: 0.6841 - dense_15_acc: 0.6844 - dense_16_acc: 0.7566 - dense_17_acc: 0.8646 - dense_18_acc: 0.7503 - dense_19_acc: 0.8288 - val_loss: 11.0667 - val_dense_loss: 0.3867 - val_dense_1_loss: 0.4374 - val_dense_2_loss: 0.4709 - val_dense_3_loss: 0.3977 - val_dense_4_loss: 0.6920 - val_dense_5_loss: 0.1821 - val_dense_6_loss: 0.3926 - val_dense_7_loss: 0.8356 - val_dense_8_loss: 0.5128 - val_dense_9_loss: 0.7464 - val_dense_10_loss: 0.5944 - val_dense_11_loss: 0.5650 - val_dense_12_loss: 0.7202 - val_dense_13_loss: 0.5418 - val_dense_14_loss: 0.8167 - val_dense_15_loss: 0.6743 - val_dense_16_loss: 0.6763 - val_dense_17_loss: 0.3934 - val_dense_18_loss: 0.5823 - val_dense_19_loss: 0.4482 - val_dense_acc: 0.8712 - val_dense_1_acc: 0.8408 - val_dense_2_acc: 0.8507 - val_dense_3_acc: 0.8888 - val_dense_4_acc: 0.7444 - val_dense_5_acc: 0.9564 - val_dense_6_acc: 0.8851 - val_dense_7_acc: 0.6876 - val_dense_8_acc: 0.8295 - val_dense_9_acc: 0.7038 - val_dense_10_acc: 0.7697 - val_dense_11_acc: 0.8025 - val_dense_12_acc: 0.7389 - val_dense_13_acc: 0.8076 - val_dense_14_acc: 0.6943 - val_dense_15_acc: 0.7122 - val_dense_16_acc: 0.7615 - val_dense_17_acc: 0.8680 - val_dense_18_acc: 0.7775 - val_dense_19_acc: 0.8326\n",
      "\n",
      "Epoch 00003: val_loss improved from 11.86504 to 11.06667, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 4/100\n",
      "99750/99750 [==============================] - 70s 706us/step - loss: 11.0052 - dense_loss: 0.3774 - dense_1_loss: 0.4417 - dense_2_loss: 0.4671 - dense_3_loss: 0.3987 - dense_4_loss: 0.7024 - dense_5_loss: 0.1733 - dense_6_loss: 0.3870 - dense_7_loss: 0.8276 - dense_8_loss: 0.4967 - dense_9_loss: 0.7460 - dense_10_loss: 0.6117 - dense_11_loss: 0.5422 - dense_12_loss: 0.7003 - dense_13_loss: 0.5549 - dense_14_loss: 0.8039 - dense_15_loss: 0.6699 - dense_16_loss: 0.6642 - dense_17_loss: 0.3997 - dense_18_loss: 0.5898 - dense_19_loss: 0.4508 - dense_acc: 0.8728 - dense_1_acc: 0.8356 - dense_2_acc: 0.8512 - dense_3_acc: 0.8881 - dense_4_acc: 0.7378 - dense_5_acc: 0.9565 - dense_6_acc: 0.8880 - dense_7_acc: 0.6861 - dense_8_acc: 0.8310 - dense_9_acc: 0.7058 - dense_10_acc: 0.7655 - dense_11_acc: 0.8133 - dense_12_acc: 0.7466 - dense_13_acc: 0.8055 - dense_14_acc: 0.7004 - dense_15_acc: 0.7073 - dense_16_acc: 0.7619 - dense_17_acc: 0.8718 - dense_18_acc: 0.7681 - dense_19_acc: 0.8377 - val_loss: 10.7367 - val_dense_loss: 0.3873 - val_dense_1_loss: 0.4199 - val_dense_2_loss: 0.4336 - val_dense_3_loss: 0.3749 - val_dense_4_loss: 0.6693 - val_dense_5_loss: 0.1733 - val_dense_6_loss: 0.3716 - val_dense_7_loss: 0.8060 - val_dense_8_loss: 0.4955 - val_dense_9_loss: 0.7381 - val_dense_10_loss: 0.5839 - val_dense_11_loss: 0.5417 - val_dense_12_loss: 0.6946 - val_dense_13_loss: 0.5355 - val_dense_14_loss: 0.7812 - val_dense_15_loss: 0.6452 - val_dense_16_loss: 0.6700 - val_dense_17_loss: 0.3839 - val_dense_18_loss: 0.5703 - val_dense_19_loss: 0.4610 - val_dense_acc: 0.8737 - val_dense_1_acc: 0.8465 - val_dense_2_acc: 0.8625 - val_dense_3_acc: 0.8933 - val_dense_4_acc: 0.7505 - val_dense_5_acc: 0.9562 - val_dense_6_acc: 0.8897 - val_dense_7_acc: 0.6983 - val_dense_8_acc: 0.8299 - val_dense_9_acc: 0.7065 - val_dense_10_acc: 0.7728 - val_dense_11_acc: 0.8109 - val_dense_12_acc: 0.7507 - val_dense_13_acc: 0.8137 - val_dense_14_acc: 0.7137 - val_dense_15_acc: 0.7183 - val_dense_16_acc: 0.7655 - val_dense_17_acc: 0.8766 - val_dense_18_acc: 0.7810 - val_dense_19_acc: 0.8297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 11.06667 to 10.73674, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 5/100\n",
      "99750/99750 [==============================] - 75s 756us/step - loss: 10.6731 - dense_loss: 0.3684 - dense_1_loss: 0.4326 - dense_2_loss: 0.4515 - dense_3_loss: 0.3836 - dense_4_loss: 0.6785 - dense_5_loss: 0.1668 - dense_6_loss: 0.3675 - dense_7_loss: 0.8051 - dense_8_loss: 0.4829 - dense_9_loss: 0.7292 - dense_10_loss: 0.5964 - dense_11_loss: 0.5228 - dense_12_loss: 0.6771 - dense_13_loss: 0.5387 - dense_14_loss: 0.7820 - dense_15_loss: 0.6452 - dense_16_loss: 0.6505 - dense_17_loss: 0.3914 - dense_18_loss: 0.5680 - dense_19_loss: 0.4348 - dense_acc: 0.8756 - dense_1_acc: 0.8382 - dense_2_acc: 0.8554 - dense_3_acc: 0.8915 - dense_4_acc: 0.7469 - dense_5_acc: 0.9584 - dense_6_acc: 0.8921 - dense_7_acc: 0.6965 - dense_8_acc: 0.8346 - dense_9_acc: 0.7138 - dense_10_acc: 0.7716 - dense_11_acc: 0.8197 - dense_12_acc: 0.7545 - dense_13_acc: 0.8099 - dense_14_acc: 0.7097 - dense_15_acc: 0.7215 - dense_16_acc: 0.7652 - dense_17_acc: 0.8740 - dense_18_acc: 0.7774 - dense_19_acc: 0.8425 - val_loss: 10.5287 - val_dense_loss: 0.3796 - val_dense_1_loss: 0.4234 - val_dense_2_loss: 0.4332 - val_dense_3_loss: 0.3582 - val_dense_4_loss: 0.6715 - val_dense_5_loss: 0.1680 - val_dense_6_loss: 0.3605 - val_dense_7_loss: 0.7931 - val_dense_8_loss: 0.4903 - val_dense_9_loss: 0.7164 - val_dense_10_loss: 0.5803 - val_dense_11_loss: 0.5301 - val_dense_12_loss: 0.6723 - val_dense_13_loss: 0.5204 - val_dense_14_loss: 0.7672 - val_dense_15_loss: 0.6420 - val_dense_16_loss: 0.6546 - val_dense_17_loss: 0.3810 - val_dense_18_loss: 0.5508 - val_dense_19_loss: 0.4358 - val_dense_acc: 0.8777 - val_dense_1_acc: 0.8436 - val_dense_2_acc: 0.8600 - val_dense_3_acc: 0.8960 - val_dense_4_acc: 0.7526 - val_dense_5_acc: 0.9577 - val_dense_6_acc: 0.8945 - val_dense_7_acc: 0.7030 - val_dense_8_acc: 0.8303 - val_dense_9_acc: 0.7116 - val_dense_10_acc: 0.7737 - val_dense_11_acc: 0.8124 - val_dense_12_acc: 0.7541 - val_dense_13_acc: 0.8128 - val_dense_14_acc: 0.7200 - val_dense_15_acc: 0.7246 - val_dense_16_acc: 0.7621 - val_dense_17_acc: 0.8750 - val_dense_18_acc: 0.7890 - val_dense_19_acc: 0.8419\n",
      "\n",
      "Epoch 00005: val_loss improved from 10.73674 to 10.52870, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 6/100\n",
      "99750/99750 [==============================] - 74s 741us/step - loss: 10.4305 - dense_loss: 0.3602 - dense_1_loss: 0.4249 - dense_2_loss: 0.4407 - dense_3_loss: 0.3691 - dense_4_loss: 0.6610 - dense_5_loss: 0.1608 - dense_6_loss: 0.3555 - dense_7_loss: 0.7884 - dense_8_loss: 0.4742 - dense_9_loss: 0.7139 - dense_10_loss: 0.5835 - dense_11_loss: 0.5124 - dense_12_loss: 0.6619 - dense_13_loss: 0.5286 - dense_14_loss: 0.7654 - dense_15_loss: 0.6300 - dense_16_loss: 0.6408 - dense_17_loss: 0.3827 - dense_18_loss: 0.5519 - dense_19_loss: 0.4244 - dense_acc: 0.8767 - dense_1_acc: 0.8399 - dense_2_acc: 0.8571 - dense_3_acc: 0.8931 - dense_4_acc: 0.7520 - dense_5_acc: 0.9600 - dense_6_acc: 0.8943 - dense_7_acc: 0.7025 - dense_8_acc: 0.8367 - dense_9_acc: 0.7210 - dense_10_acc: 0.7756 - dense_11_acc: 0.8227 - dense_12_acc: 0.7592 - dense_13_acc: 0.8135 - dense_14_acc: 0.7158 - dense_15_acc: 0.7271 - dense_16_acc: 0.7670 - dense_17_acc: 0.8761 - dense_18_acc: 0.7856 - dense_19_acc: 0.8459 - val_loss: 10.4338 - val_dense_loss: 0.3700 - val_dense_1_loss: 0.4176 - val_dense_2_loss: 0.4223 - val_dense_3_loss: 0.3490 - val_dense_4_loss: 0.6440 - val_dense_5_loss: 0.1676 - val_dense_6_loss: 0.3604 - val_dense_7_loss: 0.7978 - val_dense_8_loss: 0.4848 - val_dense_9_loss: 0.7403 - val_dense_10_loss: 0.5742 - val_dense_11_loss: 0.5259 - val_dense_12_loss: 0.6706 - val_dense_13_loss: 0.5351 - val_dense_14_loss: 0.7588 - val_dense_15_loss: 0.6297 - val_dense_16_loss: 0.6486 - val_dense_17_loss: 0.3674 - val_dense_18_loss: 0.5456 - val_dense_19_loss: 0.4242 - val_dense_acc: 0.8747 - val_dense_1_acc: 0.8448 - val_dense_2_acc: 0.8613 - val_dense_3_acc: 0.8973 - val_dense_4_acc: 0.7581 - val_dense_5_acc: 0.9590 - val_dense_6_acc: 0.8958 - val_dense_7_acc: 0.6956 - val_dense_8_acc: 0.8326 - val_dense_9_acc: 0.7120 - val_dense_10_acc: 0.7773 - val_dense_11_acc: 0.8185 - val_dense_12_acc: 0.7579 - val_dense_13_acc: 0.8067 - val_dense_14_acc: 0.7190 - val_dense_15_acc: 0.7250 - val_dense_16_acc: 0.7604 - val_dense_17_acc: 0.8792 - val_dense_18_acc: 0.7899 - val_dense_19_acc: 0.8478\n",
      "\n",
      "Epoch 00006: val_loss improved from 10.52870 to 10.43381, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 7/100\n",
      "99750/99750 [==============================] - 76s 761us/step - loss: 10.2354 - dense_loss: 0.3546 - dense_1_loss: 0.4195 - dense_2_loss: 0.4328 - dense_3_loss: 0.3602 - dense_4_loss: 0.6474 - dense_5_loss: 0.1572 - dense_6_loss: 0.3477 - dense_7_loss: 0.7766 - dense_8_loss: 0.4667 - dense_9_loss: 0.7002 - dense_10_loss: 0.5743 - dense_11_loss: 0.5007 - dense_12_loss: 0.6481 - dense_13_loss: 0.5171 - dense_14_loss: 0.7528 - dense_15_loss: 0.6165 - dense_16_loss: 0.6308 - dense_17_loss: 0.3750 - dense_18_loss: 0.5429 - dense_19_loss: 0.4143 - dense_acc: 0.8785 - dense_1_acc: 0.8400 - dense_2_acc: 0.8585 - dense_3_acc: 0.8949 - dense_4_acc: 0.7581 - dense_5_acc: 0.9606 - dense_6_acc: 0.8973 - dense_7_acc: 0.7080 - dense_8_acc: 0.8390 - dense_9_acc: 0.7263 - dense_10_acc: 0.7791 - dense_11_acc: 0.8249 - dense_12_acc: 0.7635 - dense_13_acc: 0.8171 - dense_14_acc: 0.7210 - dense_15_acc: 0.7337 - dense_16_acc: 0.7695 - dense_17_acc: 0.8784 - dense_18_acc: 0.7890 - dense_19_acc: 0.8494 - val_loss: 10.2856 - val_dense_loss: 0.3701 - val_dense_1_loss: 0.4172 - val_dense_2_loss: 0.4220 - val_dense_3_loss: 0.3447 - val_dense_4_loss: 0.6400 - val_dense_5_loss: 0.1646 - val_dense_6_loss: 0.3493 - val_dense_7_loss: 0.7882 - val_dense_8_loss: 0.4836 - val_dense_9_loss: 0.7007 - val_dense_10_loss: 0.5635 - val_dense_11_loss: 0.5220 - val_dense_12_loss: 0.6606 - val_dense_13_loss: 0.5096 - val_dense_14_loss: 0.7443 - val_dense_15_loss: 0.6290 - val_dense_16_loss: 0.6537 - val_dense_17_loss: 0.3653 - val_dense_18_loss: 0.5429 - val_dense_19_loss: 0.4144 - val_dense_acc: 0.8779 - val_dense_1_acc: 0.8461 - val_dense_2_acc: 0.8661 - val_dense_3_acc: 0.8985 - val_dense_4_acc: 0.7636 - val_dense_5_acc: 0.9585 - val_dense_6_acc: 0.8960 - val_dense_7_acc: 0.7044 - val_dense_8_acc: 0.8303 - val_dense_9_acc: 0.7238 - val_dense_10_acc: 0.7830 - val_dense_11_acc: 0.8173 - val_dense_12_acc: 0.7625 - val_dense_13_acc: 0.8198 - val_dense_14_acc: 0.7255 - val_dense_15_acc: 0.7328 - val_dense_16_acc: 0.7674 - val_dense_17_acc: 0.8798 - val_dense_18_acc: 0.7907 - val_dense_19_acc: 0.8470\n",
      "\n",
      "Epoch 00007: val_loss improved from 10.43381 to 10.28558, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 8/100\n",
      "99750/99750 [==============================] - 78s 786us/step - loss: 10.0928 - dense_loss: 0.3488 - dense_1_loss: 0.4150 - dense_2_loss: 0.4263 - dense_3_loss: 0.3546 - dense_4_loss: 0.6369 - dense_5_loss: 0.1530 - dense_6_loss: 0.3408 - dense_7_loss: 0.7684 - dense_8_loss: 0.4610 - dense_9_loss: 0.6910 - dense_10_loss: 0.5658 - dense_11_loss: 0.4945 - dense_12_loss: 0.6392 - dense_13_loss: 0.5108 - dense_14_loss: 0.7398 - dense_15_loss: 0.6088 - dense_16_loss: 0.6252 - dense_17_loss: 0.3687 - dense_18_loss: 0.5337 - dense_19_loss: 0.4108 - dense_acc: 0.8796 - dense_1_acc: 0.8414 - dense_2_acc: 0.8603 - dense_3_acc: 0.8958 - dense_4_acc: 0.7619 - dense_5_acc: 0.9614 - dense_6_acc: 0.8984 - dense_7_acc: 0.7102 - dense_8_acc: 0.8402 - dense_9_acc: 0.7311 - dense_10_acc: 0.7822 - dense_11_acc: 0.8261 - dense_12_acc: 0.7652 - dense_13_acc: 0.8180 - dense_14_acc: 0.7235 - dense_15_acc: 0.7390 - dense_16_acc: 0.7717 - dense_17_acc: 0.8801 - dense_18_acc: 0.7941 - dense_19_acc: 0.8504 - val_loss: 10.2569 - val_dense_loss: 0.3602 - val_dense_1_loss: 0.4194 - val_dense_2_loss: 0.4222 - val_dense_3_loss: 0.3434 - val_dense_4_loss: 0.6435 - val_dense_5_loss: 0.1635 - val_dense_6_loss: 0.3409 - val_dense_7_loss: 0.7825 - val_dense_8_loss: 0.4888 - val_dense_9_loss: 0.7022 - val_dense_10_loss: 0.5678 - val_dense_11_loss: 0.5179 - val_dense_12_loss: 0.6682 - val_dense_13_loss: 0.5141 - val_dense_14_loss: 0.7449 - val_dense_15_loss: 0.6235 - val_dense_16_loss: 0.6451 - val_dense_17_loss: 0.3613 - val_dense_18_loss: 0.5358 - val_dense_19_loss: 0.4118 - val_dense_acc: 0.8771 - val_dense_1_acc: 0.8461 - val_dense_2_acc: 0.8650 - val_dense_3_acc: 0.8985 - val_dense_4_acc: 0.7590 - val_dense_5_acc: 0.9594 - val_dense_6_acc: 0.8950 - val_dense_7_acc: 0.7084 - val_dense_8_acc: 0.8354 - val_dense_9_acc: 0.7274 - val_dense_10_acc: 0.7790 - val_dense_11_acc: 0.8166 - val_dense_12_acc: 0.7577 - val_dense_13_acc: 0.8160 - val_dense_14_acc: 0.7246 - val_dense_15_acc: 0.7337 - val_dense_16_acc: 0.7655 - val_dense_17_acc: 0.8794 - val_dense_18_acc: 0.7956 - val_dense_19_acc: 0.8505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 10.28558 to 10.25694, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 9/100\n",
      "99750/99750 [==============================] - 79s 787us/step - loss: 9.9389 - dense_loss: 0.3435 - dense_1_loss: 0.4097 - dense_2_loss: 0.4203 - dense_3_loss: 0.3488 - dense_4_loss: 0.6237 - dense_5_loss: 0.1513 - dense_6_loss: 0.3333 - dense_7_loss: 0.7554 - dense_8_loss: 0.4552 - dense_9_loss: 0.6829 - dense_10_loss: 0.5571 - dense_11_loss: 0.4870 - dense_12_loss: 0.6298 - dense_13_loss: 0.5024 - dense_14_loss: 0.7314 - dense_15_loss: 0.5990 - dense_16_loss: 0.6194 - dense_17_loss: 0.3640 - dense_18_loss: 0.5247 - dense_19_loss: 0.4000 - dense_acc: 0.8815 - dense_1_acc: 0.8429 - dense_2_acc: 0.8612 - dense_3_acc: 0.8972 - dense_4_acc: 0.7670 - dense_5_acc: 0.9615 - dense_6_acc: 0.9004 - dense_7_acc: 0.7141 - dense_8_acc: 0.8412 - dense_9_acc: 0.7330 - dense_10_acc: 0.7866 - dense_11_acc: 0.8287 - dense_12_acc: 0.7696 - dense_13_acc: 0.8197 - dense_14_acc: 0.7278 - dense_15_acc: 0.7421 - dense_16_acc: 0.7733 - dense_17_acc: 0.8812 - dense_18_acc: 0.7977 - dense_19_acc: 0.8551 - val_loss: 10.2069 - val_dense_loss: 0.3645 - val_dense_1_loss: 0.4299 - val_dense_2_loss: 0.4127 - val_dense_3_loss: 0.3450 - val_dense_4_loss: 0.6443 - val_dense_5_loss: 0.1611 - val_dense_6_loss: 0.3426 - val_dense_7_loss: 0.7777 - val_dense_8_loss: 0.4758 - val_dense_9_loss: 0.6961 - val_dense_10_loss: 0.5672 - val_dense_11_loss: 0.5188 - val_dense_12_loss: 0.6549 - val_dense_13_loss: 0.5094 - val_dense_14_loss: 0.7337 - val_dense_15_loss: 0.6154 - val_dense_16_loss: 0.6461 - val_dense_17_loss: 0.3563 - val_dense_18_loss: 0.5426 - val_dense_19_loss: 0.4126 - val_dense_acc: 0.8749 - val_dense_1_acc: 0.8448 - val_dense_2_acc: 0.8644 - val_dense_3_acc: 0.8996 - val_dense_4_acc: 0.7524 - val_dense_5_acc: 0.9587 - val_dense_6_acc: 0.8962 - val_dense_7_acc: 0.7110 - val_dense_8_acc: 0.8360 - val_dense_9_acc: 0.7276 - val_dense_10_acc: 0.7802 - val_dense_11_acc: 0.8211 - val_dense_12_acc: 0.7630 - val_dense_13_acc: 0.8185 - val_dense_14_acc: 0.7278 - val_dense_15_acc: 0.7330 - val_dense_16_acc: 0.7619 - val_dense_17_acc: 0.8819 - val_dense_18_acc: 0.7928 - val_dense_19_acc: 0.8480\n",
      "\n",
      "Epoch 00009: val_loss improved from 10.25694 to 10.20689, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 10/100\n",
      "99750/99750 [==============================] - 78s 787us/step - loss: 9.8119 - dense_loss: 0.3401 - dense_1_loss: 0.4060 - dense_2_loss: 0.4143 - dense_3_loss: 0.3430 - dense_4_loss: 0.6148 - dense_5_loss: 0.1478 - dense_6_loss: 0.3262 - dense_7_loss: 0.7492 - dense_8_loss: 0.4491 - dense_9_loss: 0.6722 - dense_10_loss: 0.5498 - dense_11_loss: 0.4814 - dense_12_loss: 0.6230 - dense_13_loss: 0.4980 - dense_14_loss: 0.7212 - dense_15_loss: 0.5905 - dense_16_loss: 0.6137 - dense_17_loss: 0.3583 - dense_18_loss: 0.5186 - dense_19_loss: 0.3946 - dense_acc: 0.8815 - dense_1_acc: 0.8432 - dense_2_acc: 0.8631 - dense_3_acc: 0.8981 - dense_4_acc: 0.7698 - dense_5_acc: 0.9620 - dense_6_acc: 0.9017 - dense_7_acc: 0.7167 - dense_8_acc: 0.8428 - dense_9_acc: 0.7367 - dense_10_acc: 0.7895 - dense_11_acc: 0.8306 - dense_12_acc: 0.7715 - dense_13_acc: 0.8216 - dense_14_acc: 0.7312 - dense_15_acc: 0.7469 - dense_16_acc: 0.7750 - dense_17_acc: 0.8827 - dense_18_acc: 0.8006 - dense_19_acc: 0.8570 - val_loss: 10.2022 - val_dense_loss: 0.3569 - val_dense_1_loss: 0.4285 - val_dense_2_loss: 0.4367 - val_dense_3_loss: 0.3509 - val_dense_4_loss: 0.6404 - val_dense_5_loss: 0.1611 - val_dense_6_loss: 0.3363 - val_dense_7_loss: 0.7804 - val_dense_8_loss: 0.4724 - val_dense_9_loss: 0.6942 - val_dense_10_loss: 0.5594 - val_dense_11_loss: 0.5207 - val_dense_12_loss: 0.6553 - val_dense_13_loss: 0.5056 - val_dense_14_loss: 0.7381 - val_dense_15_loss: 0.6192 - val_dense_16_loss: 0.6427 - val_dense_17_loss: 0.3651 - val_dense_18_loss: 0.5338 - val_dense_19_loss: 0.4045 - val_dense_acc: 0.8783 - val_dense_1_acc: 0.8387 - val_dense_2_acc: 0.8486 - val_dense_3_acc: 0.9000 - val_dense_4_acc: 0.7604 - val_dense_5_acc: 0.9589 - val_dense_6_acc: 0.8970 - val_dense_7_acc: 0.7078 - val_dense_8_acc: 0.8375 - val_dense_9_acc: 0.7293 - val_dense_10_acc: 0.7811 - val_dense_11_acc: 0.8213 - val_dense_12_acc: 0.7611 - val_dense_13_acc: 0.8187 - val_dense_14_acc: 0.7291 - val_dense_15_acc: 0.7347 - val_dense_16_acc: 0.7617 - val_dense_17_acc: 0.8770 - val_dense_18_acc: 0.7973 - val_dense_19_acc: 0.8520\n",
      "\n",
      "Epoch 00010: val_loss improved from 10.20689 to 10.20222, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 11/100\n",
      "99750/99750 [==============================] - 79s 788us/step - loss: 9.6862 - dense_loss: 0.3337 - dense_1_loss: 0.4002 - dense_2_loss: 0.4093 - dense_3_loss: 0.3385 - dense_4_loss: 0.6041 - dense_5_loss: 0.1449 - dense_6_loss: 0.3213 - dense_7_loss: 0.7398 - dense_8_loss: 0.4456 - dense_9_loss: 0.6651 - dense_10_loss: 0.5442 - dense_11_loss: 0.4746 - dense_12_loss: 0.6157 - dense_13_loss: 0.4908 - dense_14_loss: 0.7137 - dense_15_loss: 0.5800 - dense_16_loss: 0.6087 - dense_17_loss: 0.3529 - dense_18_loss: 0.5129 - dense_19_loss: 0.3901 - dense_acc: 0.8826 - dense_1_acc: 0.8455 - dense_2_acc: 0.8636 - dense_3_acc: 0.8993 - dense_4_acc: 0.7728 - dense_5_acc: 0.9631 - dense_6_acc: 0.9037 - dense_7_acc: 0.7211 - dense_8_acc: 0.8440 - dense_9_acc: 0.7407 - dense_10_acc: 0.7917 - dense_11_acc: 0.8328 - dense_12_acc: 0.7728 - dense_13_acc: 0.8236 - dense_14_acc: 0.7335 - dense_15_acc: 0.7505 - dense_16_acc: 0.7763 - dense_17_acc: 0.8848 - dense_18_acc: 0.8033 - dense_19_acc: 0.8572 - val_loss: 10.1751 - val_dense_loss: 0.3573 - val_dense_1_loss: 0.4209 - val_dense_2_loss: 0.4117 - val_dense_3_loss: 0.3469 - val_dense_4_loss: 0.6441 - val_dense_5_loss: 0.1626 - val_dense_6_loss: 0.3397 - val_dense_7_loss: 0.7648 - val_dense_8_loss: 0.4703 - val_dense_9_loss: 0.7164 - val_dense_10_loss: 0.5657 - val_dense_11_loss: 0.5118 - val_dense_12_loss: 0.6598 - val_dense_13_loss: 0.5114 - val_dense_14_loss: 0.7283 - val_dense_15_loss: 0.6239 - val_dense_16_loss: 0.6418 - val_dense_17_loss: 0.3554 - val_dense_18_loss: 0.5331 - val_dense_19_loss: 0.4093 - val_dense_acc: 0.8792 - val_dense_1_acc: 0.8442 - val_dense_2_acc: 0.8648 - val_dense_3_acc: 0.8992 - val_dense_4_acc: 0.7667 - val_dense_5_acc: 0.9577 - val_dense_6_acc: 0.8992 - val_dense_7_acc: 0.7170 - val_dense_8_acc: 0.8371 - val_dense_9_acc: 0.7206 - val_dense_10_acc: 0.7867 - val_dense_11_acc: 0.8229 - val_dense_12_acc: 0.7602 - val_dense_13_acc: 0.8173 - val_dense_14_acc: 0.7314 - val_dense_15_acc: 0.7368 - val_dense_16_acc: 0.7642 - val_dense_17_acc: 0.8794 - val_dense_18_acc: 0.7945 - val_dense_19_acc: 0.8491\n",
      "\n",
      "Epoch 00011: val_loss improved from 10.20222 to 10.17509, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 12/100\n",
      "99750/99750 [==============================] - 79s 790us/step - loss: 9.5619 - dense_loss: 0.3283 - dense_1_loss: 0.3963 - dense_2_loss: 0.4026 - dense_3_loss: 0.3329 - dense_4_loss: 0.5963 - dense_5_loss: 0.1414 - dense_6_loss: 0.3151 - dense_7_loss: 0.7318 - dense_8_loss: 0.4396 - dense_9_loss: 0.6576 - dense_10_loss: 0.5367 - dense_11_loss: 0.4685 - dense_12_loss: 0.6075 - dense_13_loss: 0.4851 - dense_14_loss: 0.7049 - dense_15_loss: 0.5761 - dense_16_loss: 0.6012 - dense_17_loss: 0.3494 - dense_18_loss: 0.5074 - dense_19_loss: 0.3834 - dense_acc: 0.8844 - dense_1_acc: 0.8463 - dense_2_acc: 0.8650 - dense_3_acc: 0.9002 - dense_4_acc: 0.7758 - dense_5_acc: 0.9630 - dense_6_acc: 0.9047 - dense_7_acc: 0.7236 - dense_8_acc: 0.8455 - dense_9_acc: 0.7421 - dense_10_acc: 0.7949 - dense_11_acc: 0.8341 - dense_12_acc: 0.7754 - dense_13_acc: 0.8250 - dense_14_acc: 0.7365 - dense_15_acc: 0.7540 - dense_16_acc: 0.7784 - dense_17_acc: 0.8846 - dense_18_acc: 0.8054 - dense_19_acc: 0.8599 - val_loss: 10.1867 - val_dense_loss: 0.3617 - val_dense_1_loss: 0.4267 - val_dense_2_loss: 0.4133 - val_dense_3_loss: 0.3379 - val_dense_4_loss: 0.6433 - val_dense_5_loss: 0.1609 - val_dense_6_loss: 0.3308 - val_dense_7_loss: 0.7693 - val_dense_8_loss: 0.4756 - val_dense_9_loss: 0.7212 - val_dense_10_loss: 0.5595 - val_dense_11_loss: 0.5120 - val_dense_12_loss: 0.6516 - val_dense_13_loss: 0.5221 - val_dense_14_loss: 0.7323 - val_dense_15_loss: 0.6143 - val_dense_16_loss: 0.6432 - val_dense_17_loss: 0.3604 - val_dense_18_loss: 0.5377 - val_dense_19_loss: 0.4132 - val_dense_acc: 0.8764 - val_dense_1_acc: 0.8457 - val_dense_2_acc: 0.8672 - val_dense_3_acc: 0.9000 - val_dense_4_acc: 0.7530 - val_dense_5_acc: 0.9592 - val_dense_6_acc: 0.9010 - val_dense_7_acc: 0.7131 - val_dense_8_acc: 0.8354 - val_dense_9_acc: 0.7190 - val_dense_10_acc: 0.7806 - val_dense_11_acc: 0.8215 - val_dense_12_acc: 0.7670 - val_dense_13_acc: 0.8162 - val_dense_14_acc: 0.7312 - val_dense_15_acc: 0.7373 - val_dense_16_acc: 0.7667 - val_dense_17_acc: 0.8827 - val_dense_18_acc: 0.7937 - val_dense_19_acc: 0.8524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss did not improve from 10.17509\n",
      "Epoch 13/100\n",
      "99750/99750 [==============================] - 79s 788us/step - loss: 9.4391 - dense_loss: 0.3239 - dense_1_loss: 0.3933 - dense_2_loss: 0.3974 - dense_3_loss: 0.3284 - dense_4_loss: 0.5881 - dense_5_loss: 0.1380 - dense_6_loss: 0.3097 - dense_7_loss: 0.7222 - dense_8_loss: 0.4348 - dense_9_loss: 0.6502 - dense_10_loss: 0.5293 - dense_11_loss: 0.4649 - dense_12_loss: 0.6012 - dense_13_loss: 0.4770 - dense_14_loss: 0.6976 - dense_15_loss: 0.5667 - dense_16_loss: 0.5966 - dense_17_loss: 0.3435 - dense_18_loss: 0.4997 - dense_19_loss: 0.3768 - dense_acc: 0.8865 - dense_1_acc: 0.8469 - dense_2_acc: 0.8668 - dense_3_acc: 0.9008 - dense_4_acc: 0.7790 - dense_5_acc: 0.9635 - dense_6_acc: 0.9060 - dense_7_acc: 0.7266 - dense_8_acc: 0.8471 - dense_9_acc: 0.7447 - dense_10_acc: 0.7977 - dense_11_acc: 0.8350 - dense_12_acc: 0.7780 - dense_13_acc: 0.8268 - dense_14_acc: 0.7389 - dense_15_acc: 0.7578 - dense_16_acc: 0.7804 - dense_17_acc: 0.8860 - dense_18_acc: 0.8091 - dense_19_acc: 0.8620 - val_loss: 10.2236 - val_dense_loss: 0.3691 - val_dense_1_loss: 0.4248 - val_dense_2_loss: 0.4116 - val_dense_3_loss: 0.3401 - val_dense_4_loss: 0.6360 - val_dense_5_loss: 0.1611 - val_dense_6_loss: 0.3320 - val_dense_7_loss: 0.7664 - val_dense_8_loss: 0.4816 - val_dense_9_loss: 0.6997 - val_dense_10_loss: 0.5689 - val_dense_11_loss: 0.5199 - val_dense_12_loss: 0.6501 - val_dense_13_loss: 0.5165 - val_dense_14_loss: 0.7338 - val_dense_15_loss: 0.6233 - val_dense_16_loss: 0.6526 - val_dense_17_loss: 0.3604 - val_dense_18_loss: 0.5617 - val_dense_19_loss: 0.4139 - val_dense_acc: 0.8770 - val_dense_1_acc: 0.8429 - val_dense_2_acc: 0.8667 - val_dense_3_acc: 0.9010 - val_dense_4_acc: 0.7638 - val_dense_5_acc: 0.9583 - val_dense_6_acc: 0.9011 - val_dense_7_acc: 0.7173 - val_dense_8_acc: 0.8390 - val_dense_9_acc: 0.7270 - val_dense_10_acc: 0.7796 - val_dense_11_acc: 0.8185 - val_dense_12_acc: 0.7646 - val_dense_13_acc: 0.8189 - val_dense_14_acc: 0.7310 - val_dense_15_acc: 0.7381 - val_dense_16_acc: 0.7604 - val_dense_17_acc: 0.8834 - val_dense_18_acc: 0.7789 - val_dense_19_acc: 0.8482\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 10.17509\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/100\n",
      "99750/99750 [==============================] - 78s 786us/step - loss: 9.1421 - dense_loss: 0.3107 - dense_1_loss: 0.3795 - dense_2_loss: 0.3820 - dense_3_loss: 0.3159 - dense_4_loss: 0.5685 - dense_5_loss: 0.1290 - dense_6_loss: 0.2966 - dense_7_loss: 0.7060 - dense_8_loss: 0.4215 - dense_9_loss: 0.6322 - dense_10_loss: 0.5142 - dense_11_loss: 0.4497 - dense_12_loss: 0.5819 - dense_13_loss: 0.4635 - dense_14_loss: 0.6803 - dense_15_loss: 0.5492 - dense_16_loss: 0.5836 - dense_17_loss: 0.3305 - dense_18_loss: 0.4856 - dense_19_loss: 0.3617 - dense_acc: 0.8899 - dense_1_acc: 0.8516 - dense_2_acc: 0.8703 - dense_3_acc: 0.9034 - dense_4_acc: 0.7869 - dense_5_acc: 0.9646 - dense_6_acc: 0.9092 - dense_7_acc: 0.7334 - dense_8_acc: 0.8502 - dense_9_acc: 0.7534 - dense_10_acc: 0.8036 - dense_11_acc: 0.8392 - dense_12_acc: 0.7844 - dense_13_acc: 0.8319 - dense_14_acc: 0.7458 - dense_15_acc: 0.7670 - dense_16_acc: 0.7830 - dense_17_acc: 0.8896 - dense_18_acc: 0.8149 - dense_19_acc: 0.8676 - val_loss: 10.1421 - val_dense_loss: 0.3606 - val_dense_1_loss: 0.4291 - val_dense_2_loss: 0.4162 - val_dense_3_loss: 0.3364 - val_dense_4_loss: 0.6255 - val_dense_5_loss: 0.1688 - val_dense_6_loss: 0.3274 - val_dense_7_loss: 0.7694 - val_dense_8_loss: 0.4745 - val_dense_9_loss: 0.6888 - val_dense_10_loss: 0.5624 - val_dense_11_loss: 0.5149 - val_dense_12_loss: 0.6544 - val_dense_13_loss: 0.5133 - val_dense_14_loss: 0.7300 - val_dense_15_loss: 0.6150 - val_dense_16_loss: 0.6553 - val_dense_17_loss: 0.3583 - val_dense_18_loss: 0.5349 - val_dense_19_loss: 0.4067 - val_dense_acc: 0.8792 - val_dense_1_acc: 0.8436 - val_dense_2_acc: 0.8699 - val_dense_3_acc: 0.9006 - val_dense_4_acc: 0.7640 - val_dense_5_acc: 0.9590 - val_dense_6_acc: 0.9025 - val_dense_7_acc: 0.7190 - val_dense_8_acc: 0.8366 - val_dense_9_acc: 0.7324 - val_dense_10_acc: 0.7810 - val_dense_11_acc: 0.8192 - val_dense_12_acc: 0.7684 - val_dense_13_acc: 0.8171 - val_dense_14_acc: 0.7349 - val_dense_15_acc: 0.7356 - val_dense_16_acc: 0.7630 - val_dense_17_acc: 0.8836 - val_dense_18_acc: 0.7979 - val_dense_19_acc: 0.8486\n",
      "\n",
      "Epoch 00014: val_loss improved from 10.17509 to 10.14208, saving model to ../ckpt/best_model_bigru_cnn_2.hdf5\n",
      "Epoch 15/100\n",
      "99750/99750 [==============================] - 80s 800us/step - loss: 9.0351 - dense_loss: 0.3063 - dense_1_loss: 0.3757 - dense_2_loss: 0.3786 - dense_3_loss: 0.3110 - dense_4_loss: 0.5627 - dense_5_loss: 0.1263 - dense_6_loss: 0.2909 - dense_7_loss: 0.6997 - dense_8_loss: 0.4145 - dense_9_loss: 0.6247 - dense_10_loss: 0.5064 - dense_11_loss: 0.4433 - dense_12_loss: 0.5768 - dense_13_loss: 0.4577 - dense_14_loss: 0.6737 - dense_15_loss: 0.5438 - dense_16_loss: 0.5780 - dense_17_loss: 0.3271 - dense_18_loss: 0.4800 - dense_19_loss: 0.3580 - dense_acc: 0.8910 - dense_1_acc: 0.8531 - dense_2_acc: 0.8708 - dense_3_acc: 0.9048 - dense_4_acc: 0.7873 - dense_5_acc: 0.9656 - dense_6_acc: 0.9101 - dense_7_acc: 0.7346 - dense_8_acc: 0.8526 - dense_9_acc: 0.7551 - dense_10_acc: 0.8065 - dense_11_acc: 0.8419 - dense_12_acc: 0.7855 - dense_13_acc: 0.8330 - dense_14_acc: 0.7479 - dense_15_acc: 0.7699 - dense_16_acc: 0.7848 - dense_17_acc: 0.8898 - dense_18_acc: 0.8180 - dense_19_acc: 0.8685 - val_loss: 10.1971 - val_dense_loss: 0.3619 - val_dense_1_loss: 0.4369 - val_dense_2_loss: 0.4183 - val_dense_3_loss: 0.3387 - val_dense_4_loss: 0.6339 - val_dense_5_loss: 0.1649 - val_dense_6_loss: 0.3299 - val_dense_7_loss: 0.7758 - val_dense_8_loss: 0.4832 - val_dense_9_loss: 0.6962 - val_dense_10_loss: 0.5697 - val_dense_11_loss: 0.5152 - val_dense_12_loss: 0.6486 - val_dense_13_loss: 0.5117 - val_dense_14_loss: 0.7282 - val_dense_15_loss: 0.6172 - val_dense_16_loss: 0.6440 - val_dense_17_loss: 0.3703 - val_dense_18_loss: 0.5463 - val_dense_19_loss: 0.4062 - val_dense_acc: 0.8800 - val_dense_1_acc: 0.8295 - val_dense_2_acc: 0.8655 - val_dense_3_acc: 0.9000 - val_dense_4_acc: 0.7610 - val_dense_5_acc: 0.9589 - val_dense_6_acc: 0.9013 - val_dense_7_acc: 0.7156 - val_dense_8_acc: 0.8343 - val_dense_9_acc: 0.7326 - val_dense_10_acc: 0.7779 - val_dense_11_acc: 0.8190 - val_dense_12_acc: 0.7632 - val_dense_13_acc: 0.8168 - val_dense_14_acc: 0.7362 - val_dense_15_acc: 0.7379 - val_dense_16_acc: 0.7608 - val_dense_17_acc: 0.8785 - val_dense_18_acc: 0.7914 - val_dense_19_acc: 0.8490\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 10.14208\n",
      "Epoch 16/100\n",
      "99750/99750 [==============================] - 78s 780us/step - loss: 8.9470 - dense_loss: 0.3022 - dense_1_loss: 0.3714 - dense_2_loss: 0.3726 - dense_3_loss: 0.3077 - dense_4_loss: 0.5576 - dense_5_loss: 0.1227 - dense_6_loss: 0.2875 - dense_7_loss: 0.6934 - dense_8_loss: 0.4119 - dense_9_loss: 0.6209 - dense_10_loss: 0.5030 - dense_11_loss: 0.4394 - dense_12_loss: 0.5702 - dense_13_loss: 0.4531 - dense_14_loss: 0.6684 - dense_15_loss: 0.5385 - dense_16_loss: 0.5729 - dense_17_loss: 0.3224 - dense_18_loss: 0.4755 - dense_19_loss: 0.3557 - dense_acc: 0.8920 - dense_1_acc: 0.8535 - dense_2_acc: 0.8722 - dense_3_acc: 0.9056 - dense_4_acc: 0.7899 - dense_5_acc: 0.9659 - dense_6_acc: 0.9120 - dense_7_acc: 0.7378 - dense_8_acc: 0.8530 - dense_9_acc: 0.7578 - dense_10_acc: 0.8077 - dense_11_acc: 0.8426 - dense_12_acc: 0.7889 - dense_13_acc: 0.8343 - dense_14_acc: 0.7504 - dense_15_acc: 0.7716 - dense_16_acc: 0.7864 - dense_17_acc: 0.8915 - dense_18_acc: 0.8194 - dense_19_acc: 0.8692 - val_loss: 10.2215 - val_dense_loss: 0.3670 - val_dense_1_loss: 0.4355 - val_dense_2_loss: 0.4243 - val_dense_3_loss: 0.3438 - val_dense_4_loss: 0.6283 - val_dense_5_loss: 0.1656 - val_dense_6_loss: 0.3316 - val_dense_7_loss: 0.7668 - val_dense_8_loss: 0.4832 - val_dense_9_loss: 0.6924 - val_dense_10_loss: 0.5658 - val_dense_11_loss: 0.5192 - val_dense_12_loss: 0.6547 - val_dense_13_loss: 0.5142 - val_dense_14_loss: 0.7415 - val_dense_15_loss: 0.6191 - val_dense_16_loss: 0.6445 - val_dense_17_loss: 0.3594 - val_dense_18_loss: 0.5503 - val_dense_19_loss: 0.4144 - val_dense_acc: 0.8762 - val_dense_1_acc: 0.8373 - val_dense_2_acc: 0.8592 - val_dense_3_acc: 0.8977 - val_dense_4_acc: 0.7655 - val_dense_5_acc: 0.9592 - val_dense_6_acc: 0.9017 - val_dense_7_acc: 0.7206 - val_dense_8_acc: 0.8326 - val_dense_9_acc: 0.7309 - val_dense_10_acc: 0.7794 - val_dense_11_acc: 0.8179 - val_dense_12_acc: 0.7642 - val_dense_13_acc: 0.8189 - val_dense_14_acc: 0.7286 - val_dense_15_acc: 0.7427 - val_dense_16_acc: 0.7604 - val_dense_17_acc: 0.8836 - val_dense_18_acc: 0.7897 - val_dense_19_acc: 0.8455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: val_loss did not improve from 10.14208\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 17/100\n",
      "99750/99750 [==============================] - 79s 796us/step - loss: 8.7462 - dense_loss: 0.2937 - dense_1_loss: 0.3643 - dense_2_loss: 0.3644 - dense_3_loss: 0.2986 - dense_4_loss: 0.5445 - dense_5_loss: 0.1167 - dense_6_loss: 0.2790 - dense_7_loss: 0.6805 - dense_8_loss: 0.4022 - dense_9_loss: 0.6094 - dense_10_loss: 0.4898 - dense_11_loss: 0.4282 - dense_12_loss: 0.5594 - dense_13_loss: 0.4422 - dense_14_loss: 0.6569 - dense_15_loss: 0.5271 - dense_16_loss: 0.5620 - dense_17_loss: 0.3139 - dense_18_loss: 0.4670 - dense_19_loss: 0.3465 - dense_acc: 0.8942 - dense_1_acc: 0.8561 - dense_2_acc: 0.8743 - dense_3_acc: 0.9075 - dense_4_acc: 0.7948 - dense_5_acc: 0.9671 - dense_6_acc: 0.9135 - dense_7_acc: 0.7434 - dense_8_acc: 0.8563 - dense_9_acc: 0.7625 - dense_10_acc: 0.8142 - dense_11_acc: 0.8465 - dense_12_acc: 0.7927 - dense_13_acc: 0.8380 - dense_14_acc: 0.7536 - dense_15_acc: 0.7773 - dense_16_acc: 0.7902 - dense_17_acc: 0.8947 - dense_18_acc: 0.8237 - dense_19_acc: 0.8727 - val_loss: 10.2263 - val_dense_loss: 0.3630 - val_dense_1_loss: 0.4367 - val_dense_2_loss: 0.4189 - val_dense_3_loss: 0.3418 - val_dense_4_loss: 0.6337 - val_dense_5_loss: 0.1661 - val_dense_6_loss: 0.3318 - val_dense_7_loss: 0.7679 - val_dense_8_loss: 0.4850 - val_dense_9_loss: 0.6961 - val_dense_10_loss: 0.5718 - val_dense_11_loss: 0.5227 - val_dense_12_loss: 0.6534 - val_dense_13_loss: 0.5180 - val_dense_14_loss: 0.7365 - val_dense_15_loss: 0.6201 - val_dense_16_loss: 0.6464 - val_dense_17_loss: 0.3616 - val_dense_18_loss: 0.5430 - val_dense_19_loss: 0.4120 - val_dense_acc: 0.8783 - val_dense_1_acc: 0.8396 - val_dense_2_acc: 0.8663 - val_dense_3_acc: 0.8975 - val_dense_4_acc: 0.7638 - val_dense_5_acc: 0.9596 - val_dense_6_acc: 0.9002 - val_dense_7_acc: 0.7170 - val_dense_8_acc: 0.8375 - val_dense_9_acc: 0.7301 - val_dense_10_acc: 0.7770 - val_dense_11_acc: 0.8168 - val_dense_12_acc: 0.7663 - val_dense_13_acc: 0.8147 - val_dense_14_acc: 0.7364 - val_dense_15_acc: 0.7408 - val_dense_16_acc: 0.7596 - val_dense_17_acc: 0.8821 - val_dense_18_acc: 0.7962 - val_dense_19_acc: 0.8478\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 10.14208\n",
      "Epoch 18/100\n",
      "99750/99750 [==============================] - 80s 798us/step - loss: 8.6807 - dense_loss: 0.2912 - dense_1_loss: 0.3599 - dense_2_loss: 0.3610 - dense_3_loss: 0.2959 - dense_4_loss: 0.5407 - dense_5_loss: 0.1153 - dense_6_loss: 0.2756 - dense_7_loss: 0.6758 - dense_8_loss: 0.4007 - dense_9_loss: 0.6047 - dense_10_loss: 0.4866 - dense_11_loss: 0.4264 - dense_12_loss: 0.5562 - dense_13_loss: 0.4387 - dense_14_loss: 0.6525 - dense_15_loss: 0.5247 - dense_16_loss: 0.5591 - dense_17_loss: 0.3114 - dense_18_loss: 0.4623 - dense_19_loss: 0.3418 - dense_acc: 0.8948 - dense_1_acc: 0.8575 - dense_2_acc: 0.8748 - dense_3_acc: 0.9082 - dense_4_acc: 0.7960 - dense_5_acc: 0.9673 - dense_6_acc: 0.9134 - dense_7_acc: 0.7440 - dense_8_acc: 0.8571 - dense_9_acc: 0.7653 - dense_10_acc: 0.8157 - dense_11_acc: 0.8468 - dense_12_acc: 0.7924 - dense_13_acc: 0.8396 - dense_14_acc: 0.7544 - dense_15_acc: 0.7780 - dense_16_acc: 0.7911 - dense_17_acc: 0.8952 - dense_18_acc: 0.8249 - dense_19_acc: 0.8741 - val_loss: 10.2606 - val_dense_loss: 0.3662 - val_dense_1_loss: 0.4411 - val_dense_2_loss: 0.4207 - val_dense_3_loss: 0.3440 - val_dense_4_loss: 0.6336 - val_dense_5_loss: 0.1655 - val_dense_6_loss: 0.3368 - val_dense_7_loss: 0.7676 - val_dense_8_loss: 0.4821 - val_dense_9_loss: 0.7009 - val_dense_10_loss: 0.5712 - val_dense_11_loss: 0.5247 - val_dense_12_loss: 0.6529 - val_dense_13_loss: 0.5170 - val_dense_14_loss: 0.7374 - val_dense_15_loss: 0.6227 - val_dense_16_loss: 0.6467 - val_dense_17_loss: 0.3704 - val_dense_18_loss: 0.5447 - val_dense_19_loss: 0.4144 - val_dense_acc: 0.8790 - val_dense_1_acc: 0.8299 - val_dense_2_acc: 0.8651 - val_dense_3_acc: 0.8987 - val_dense_4_acc: 0.7598 - val_dense_5_acc: 0.9590 - val_dense_6_acc: 0.9019 - val_dense_7_acc: 0.7170 - val_dense_8_acc: 0.8371 - val_dense_9_acc: 0.7286 - val_dense_10_acc: 0.7808 - val_dense_11_acc: 0.8187 - val_dense_12_acc: 0.7627 - val_dense_13_acc: 0.8141 - val_dense_14_acc: 0.7330 - val_dense_15_acc: 0.7419 - val_dense_16_acc: 0.7604 - val_dense_17_acc: 0.8739 - val_dense_18_acc: 0.7941 - val_dense_19_acc: 0.8469\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 10.14208\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/100\n",
      "99750/99750 [==============================] - 78s 781us/step - loss: 8.5667 - dense_loss: 0.2852 - dense_1_loss: 0.3553 - dense_2_loss: 0.3559 - dense_3_loss: 0.2915 - dense_4_loss: 0.5338 - dense_5_loss: 0.1118 - dense_6_loss: 0.2714 - dense_7_loss: 0.6700 - dense_8_loss: 0.3947 - dense_9_loss: 0.5988 - dense_10_loss: 0.4804 - dense_11_loss: 0.4190 - dense_12_loss: 0.5498 - dense_13_loss: 0.4342 - dense_14_loss: 0.6460 - dense_15_loss: 0.5168 - dense_16_loss: 0.5534 - dense_17_loss: 0.3066 - dense_18_loss: 0.4557 - dense_19_loss: 0.3362 - dense_acc: 0.8974 - dense_1_acc: 0.8594 - dense_2_acc: 0.8768 - dense_3_acc: 0.9090 - dense_4_acc: 0.7984 - dense_5_acc: 0.9680 - dense_6_acc: 0.9149 - dense_7_acc: 0.7457 - dense_8_acc: 0.8583 - dense_9_acc: 0.7660 - dense_10_acc: 0.8181 - dense_11_acc: 0.8491 - dense_12_acc: 0.7948 - dense_13_acc: 0.8410 - dense_14_acc: 0.7581 - dense_15_acc: 0.7815 - dense_16_acc: 0.7929 - dense_17_acc: 0.8965 - dense_18_acc: 0.8283 - dense_19_acc: 0.8771 - val_loss: 10.2501 - val_dense_loss: 0.3651 - val_dense_1_loss: 0.4400 - val_dense_2_loss: 0.4230 - val_dense_3_loss: 0.3435 - val_dense_4_loss: 0.6321 - val_dense_5_loss: 0.1665 - val_dense_6_loss: 0.3361 - val_dense_7_loss: 0.7725 - val_dense_8_loss: 0.4838 - val_dense_9_loss: 0.6962 - val_dense_10_loss: 0.5716 - val_dense_11_loss: 0.5211 - val_dense_12_loss: 0.6559 - val_dense_13_loss: 0.5189 - val_dense_14_loss: 0.7342 - val_dense_15_loss: 0.6231 - val_dense_16_loss: 0.6472 - val_dense_17_loss: 0.3644 - val_dense_18_loss: 0.5429 - val_dense_19_loss: 0.4118 - val_dense_acc: 0.8798 - val_dense_1_acc: 0.8370 - val_dense_2_acc: 0.8625 - val_dense_3_acc: 0.8985 - val_dense_4_acc: 0.7644 - val_dense_5_acc: 0.9594 - val_dense_6_acc: 0.9021 - val_dense_7_acc: 0.7128 - val_dense_8_acc: 0.8377 - val_dense_9_acc: 0.7301 - val_dense_10_acc: 0.7789 - val_dense_11_acc: 0.8194 - val_dense_12_acc: 0.7636 - val_dense_13_acc: 0.8137 - val_dense_14_acc: 0.7370 - val_dense_15_acc: 0.7392 - val_dense_16_acc: 0.7596 - val_dense_17_acc: 0.8815 - val_dense_18_acc: 0.7949 - val_dense_19_acc: 0.8469\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 10.14208\n",
      "Epoch 20/100\n",
      "99750/99750 [==============================] - 75s 748us/step - loss: 8.5297 - dense_loss: 0.2839 - dense_1_loss: 0.3538 - dense_2_loss: 0.3545 - dense_3_loss: 0.2889 - dense_4_loss: 0.5335 - dense_5_loss: 0.1101 - dense_6_loss: 0.2691 - dense_7_loss: 0.6679 - dense_8_loss: 0.3929 - dense_9_loss: 0.5965 - dense_10_loss: 0.4779 - dense_11_loss: 0.4171 - dense_12_loss: 0.5462 - dense_13_loss: 0.4312 - dense_14_loss: 0.6430 - dense_15_loss: 0.5166 - dense_16_loss: 0.5526 - dense_17_loss: 0.3050 - dense_18_loss: 0.4541 - dense_19_loss: 0.3349 - dense_acc: 0.8974 - dense_1_acc: 0.8598 - dense_2_acc: 0.8777 - dense_3_acc: 0.9094 - dense_4_acc: 0.7986 - dense_5_acc: 0.9684 - dense_6_acc: 0.9153 - dense_7_acc: 0.7470 - dense_8_acc: 0.8591 - dense_9_acc: 0.7665 - dense_10_acc: 0.8191 - dense_11_acc: 0.8498 - dense_12_acc: 0.7959 - dense_13_acc: 0.8416 - dense_14_acc: 0.7588 - dense_15_acc: 0.7822 - dense_16_acc: 0.7928 - dense_17_acc: 0.8960 - dense_18_acc: 0.8283 - dense_19_acc: 0.8757 - val_loss: 10.2661 - val_dense_loss: 0.3642 - val_dense_1_loss: 0.4410 - val_dense_2_loss: 0.4242 - val_dense_3_loss: 0.3440 - val_dense_4_loss: 0.6330 - val_dense_5_loss: 0.1671 - val_dense_6_loss: 0.3354 - val_dense_7_loss: 0.7686 - val_dense_8_loss: 0.4833 - val_dense_9_loss: 0.6975 - val_dense_10_loss: 0.5718 - val_dense_11_loss: 0.5250 - val_dense_12_loss: 0.6549 - val_dense_13_loss: 0.5207 - val_dense_14_loss: 0.7382 - val_dense_15_loss: 0.6223 - val_dense_16_loss: 0.6547 - val_dense_17_loss: 0.3634 - val_dense_18_loss: 0.5434 - val_dense_19_loss: 0.4135 - val_dense_acc: 0.8794 - val_dense_1_acc: 0.8322 - val_dense_2_acc: 0.8610 - val_dense_3_acc: 0.8970 - val_dense_4_acc: 0.7636 - val_dense_5_acc: 0.9594 - val_dense_6_acc: 0.9023 - val_dense_7_acc: 0.7173 - val_dense_8_acc: 0.8385 - val_dense_9_acc: 0.7316 - val_dense_10_acc: 0.7792 - val_dense_11_acc: 0.8196 - val_dense_12_acc: 0.7632 - val_dense_13_acc: 0.8145 - val_dense_14_acc: 0.7373 - val_dense_15_acc: 0.7392 - val_dense_16_acc: 0.7598 - val_dense_17_acc: 0.8827 - val_dense_18_acc: 0.7945 - val_dense_19_acc: 0.8484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss did not improve from 10.14208\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/100\n",
      "99750/99750 [==============================] - 74s 740us/step - loss: 8.4746 - dense_loss: 0.2816 - dense_1_loss: 0.3504 - dense_2_loss: 0.3514 - dense_3_loss: 0.2872 - dense_4_loss: 0.5278 - dense_5_loss: 0.1080 - dense_6_loss: 0.2671 - dense_7_loss: 0.6640 - dense_8_loss: 0.3907 - dense_9_loss: 0.5922 - dense_10_loss: 0.4752 - dense_11_loss: 0.4148 - dense_12_loss: 0.5447 - dense_13_loss: 0.4290 - dense_14_loss: 0.6409 - dense_15_loss: 0.5133 - dense_16_loss: 0.5497 - dense_17_loss: 0.3031 - dense_18_loss: 0.4519 - dense_19_loss: 0.3317 - dense_acc: 0.8973 - dense_1_acc: 0.8616 - dense_2_acc: 0.8778 - dense_3_acc: 0.9103 - dense_4_acc: 0.8020 - dense_5_acc: 0.9687 - dense_6_acc: 0.9156 - dense_7_acc: 0.7484 - dense_8_acc: 0.8594 - dense_9_acc: 0.7687 - dense_10_acc: 0.8203 - dense_11_acc: 0.8509 - dense_12_acc: 0.7977 - dense_13_acc: 0.8423 - dense_14_acc: 0.7587 - dense_15_acc: 0.7842 - dense_16_acc: 0.7937 - dense_17_acc: 0.8970 - dense_18_acc: 0.8293 - dense_19_acc: 0.8775 - val_loss: 10.2793 - val_dense_loss: 0.3646 - val_dense_1_loss: 0.4415 - val_dense_2_loss: 0.4279 - val_dense_3_loss: 0.3455 - val_dense_4_loss: 0.6335 - val_dense_5_loss: 0.1665 - val_dense_6_loss: 0.3347 - val_dense_7_loss: 0.7696 - val_dense_8_loss: 0.4835 - val_dense_9_loss: 0.7002 - val_dense_10_loss: 0.5716 - val_dense_11_loss: 0.5255 - val_dense_12_loss: 0.6553 - val_dense_13_loss: 0.5209 - val_dense_14_loss: 0.7390 - val_dense_15_loss: 0.6279 - val_dense_16_loss: 0.6494 - val_dense_17_loss: 0.3653 - val_dense_18_loss: 0.5441 - val_dense_19_loss: 0.4126 - val_dense_acc: 0.8770 - val_dense_1_acc: 0.8316 - val_dense_2_acc: 0.8545 - val_dense_3_acc: 0.8979 - val_dense_4_acc: 0.7630 - val_dense_5_acc: 0.9592 - val_dense_6_acc: 0.9023 - val_dense_7_acc: 0.7154 - val_dense_8_acc: 0.8356 - val_dense_9_acc: 0.7278 - val_dense_10_acc: 0.7798 - val_dense_11_acc: 0.8175 - val_dense_12_acc: 0.7636 - val_dense_13_acc: 0.8145 - val_dense_14_acc: 0.7341 - val_dense_15_acc: 0.7398 - val_dense_16_acc: 0.7594 - val_dense_17_acc: 0.8806 - val_dense_18_acc: 0.7958 - val_dense_19_acc: 0.8463\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 10.14208\n",
      "Epoch 22/100\n",
      "99750/99750 [==============================] - 75s 749us/step - loss: 8.4552 - dense_loss: 0.2805 - dense_1_loss: 0.3489 - dense_2_loss: 0.3502 - dense_3_loss: 0.2860 - dense_4_loss: 0.5267 - dense_5_loss: 0.1077 - dense_6_loss: 0.2666 - dense_7_loss: 0.6622 - dense_8_loss: 0.3893 - dense_9_loss: 0.5928 - dense_10_loss: 0.4746 - dense_11_loss: 0.4141 - dense_12_loss: 0.5424 - dense_13_loss: 0.4287 - dense_14_loss: 0.6395 - dense_15_loss: 0.5121 - dense_16_loss: 0.5481 - dense_17_loss: 0.3023 - dense_18_loss: 0.4510 - dense_19_loss: 0.3317 - dense_acc: 0.8985 - dense_1_acc: 0.8606 - dense_2_acc: 0.8778 - dense_3_acc: 0.9102 - dense_4_acc: 0.8020 - dense_5_acc: 0.9686 - dense_6_acc: 0.9157 - dense_7_acc: 0.7488 - dense_8_acc: 0.8598 - dense_9_acc: 0.7681 - dense_10_acc: 0.8205 - dense_11_acc: 0.8507 - dense_12_acc: 0.7975 - dense_13_acc: 0.8418 - dense_14_acc: 0.7596 - dense_15_acc: 0.7839 - dense_16_acc: 0.7925 - dense_17_acc: 0.8973 - dense_18_acc: 0.8297 - dense_19_acc: 0.8773 - val_loss: 10.2842 - val_dense_loss: 0.3668 - val_dense_1_loss: 0.4423 - val_dense_2_loss: 0.4260 - val_dense_3_loss: 0.3455 - val_dense_4_loss: 0.6327 - val_dense_5_loss: 0.1676 - val_dense_6_loss: 0.3361 - val_dense_7_loss: 0.7700 - val_dense_8_loss: 0.4842 - val_dense_9_loss: 0.6994 - val_dense_10_loss: 0.5727 - val_dense_11_loss: 0.5244 - val_dense_12_loss: 0.6560 - val_dense_13_loss: 0.5221 - val_dense_14_loss: 0.7383 - val_dense_15_loss: 0.6260 - val_dense_16_loss: 0.6506 - val_dense_17_loss: 0.3649 - val_dense_18_loss: 0.5442 - val_dense_19_loss: 0.4143 - val_dense_acc: 0.8804 - val_dense_1_acc: 0.8328 - val_dense_2_acc: 0.8610 - val_dense_3_acc: 0.8971 - val_dense_4_acc: 0.7648 - val_dense_5_acc: 0.9594 - val_dense_6_acc: 0.9013 - val_dense_7_acc: 0.7181 - val_dense_8_acc: 0.8354 - val_dense_9_acc: 0.7297 - val_dense_10_acc: 0.7771 - val_dense_11_acc: 0.8179 - val_dense_12_acc: 0.7627 - val_dense_13_acc: 0.8118 - val_dense_14_acc: 0.7356 - val_dense_15_acc: 0.7394 - val_dense_16_acc: 0.7596 - val_dense_17_acc: 0.8821 - val_dense_18_acc: 0.7947 - val_dense_19_acc: 0.8467\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 10.14208\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 23/100\n",
      "99750/99750 [==============================] - 76s 761us/step - loss: 8.4252 - dense_loss: 0.2789 - dense_1_loss: 0.3480 - dense_2_loss: 0.3490 - dense_3_loss: 0.2850 - dense_4_loss: 0.5256 - dense_5_loss: 0.1072 - dense_6_loss: 0.2640 - dense_7_loss: 0.6608 - dense_8_loss: 0.3883 - dense_9_loss: 0.5896 - dense_10_loss: 0.4720 - dense_11_loss: 0.4130 - dense_12_loss: 0.5411 - dense_13_loss: 0.4273 - dense_14_loss: 0.6377 - dense_15_loss: 0.5114 - dense_16_loss: 0.5462 - dense_17_loss: 0.3004 - dense_18_loss: 0.4495 - dense_19_loss: 0.3304 - dense_acc: 0.8992 - dense_1_acc: 0.8623 - dense_2_acc: 0.8785 - dense_3_acc: 0.9103 - dense_4_acc: 0.8011 - dense_5_acc: 0.9688 - dense_6_acc: 0.9162 - dense_7_acc: 0.7495 - dense_8_acc: 0.8603 - dense_9_acc: 0.7702 - dense_10_acc: 0.8223 - dense_11_acc: 0.8510 - dense_12_acc: 0.7971 - dense_13_acc: 0.8432 - dense_14_acc: 0.7592 - dense_15_acc: 0.7841 - dense_16_acc: 0.7950 - dense_17_acc: 0.8984 - dense_18_acc: 0.8315 - dense_19_acc: 0.8782 - val_loss: 10.2941 - val_dense_loss: 0.3654 - val_dense_1_loss: 0.4466 - val_dense_2_loss: 0.4263 - val_dense_3_loss: 0.3453 - val_dense_4_loss: 0.6340 - val_dense_5_loss: 0.1672 - val_dense_6_loss: 0.3361 - val_dense_7_loss: 0.7698 - val_dense_8_loss: 0.4850 - val_dense_9_loss: 0.6998 - val_dense_10_loss: 0.5728 - val_dense_11_loss: 0.5255 - val_dense_12_loss: 0.6575 - val_dense_13_loss: 0.5221 - val_dense_14_loss: 0.7381 - val_dense_15_loss: 0.6268 - val_dense_16_loss: 0.6510 - val_dense_17_loss: 0.3649 - val_dense_18_loss: 0.5460 - val_dense_19_loss: 0.4136 - val_dense_acc: 0.8779 - val_dense_1_acc: 0.8267 - val_dense_2_acc: 0.8598 - val_dense_3_acc: 0.8983 - val_dense_4_acc: 0.7644 - val_dense_5_acc: 0.9590 - val_dense_6_acc: 0.9015 - val_dense_7_acc: 0.7173 - val_dense_8_acc: 0.8370 - val_dense_9_acc: 0.7310 - val_dense_10_acc: 0.7800 - val_dense_11_acc: 0.8185 - val_dense_12_acc: 0.7627 - val_dense_13_acc: 0.8141 - val_dense_14_acc: 0.7356 - val_dense_15_acc: 0.7394 - val_dense_16_acc: 0.7585 - val_dense_17_acc: 0.8819 - val_dense_18_acc: 0.7958 - val_dense_19_acc: 0.8463\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 10.14208\n",
      "Epoch 24/100\n",
      "99750/99750 [==============================] - 73s 729us/step - loss: 8.4089 - dense_loss: 0.2786 - dense_1_loss: 0.3471 - dense_2_loss: 0.3479 - dense_3_loss: 0.2845 - dense_4_loss: 0.5256 - dense_5_loss: 0.1067 - dense_6_loss: 0.2639 - dense_7_loss: 0.6598 - dense_8_loss: 0.3873 - dense_9_loss: 0.5885 - dense_10_loss: 0.4721 - dense_11_loss: 0.4113 - dense_12_loss: 0.5404 - dense_13_loss: 0.4250 - dense_14_loss: 0.6375 - dense_15_loss: 0.5099 - dense_16_loss: 0.5451 - dense_17_loss: 0.2998 - dense_18_loss: 0.4482 - dense_19_loss: 0.3299 - dense_acc: 0.8991 - dense_1_acc: 0.8620 - dense_2_acc: 0.8785 - dense_3_acc: 0.9104 - dense_4_acc: 0.8017 - dense_5_acc: 0.9688 - dense_6_acc: 0.9162 - dense_7_acc: 0.7500 - dense_8_acc: 0.8602 - dense_9_acc: 0.7710 - dense_10_acc: 0.8220 - dense_11_acc: 0.8516 - dense_12_acc: 0.7978 - dense_13_acc: 0.8440 - dense_14_acc: 0.7604 - dense_15_acc: 0.7857 - dense_16_acc: 0.7948 - dense_17_acc: 0.8983 - dense_18_acc: 0.8321 - dense_19_acc: 0.8787 - val_loss: 10.2939 - val_dense_loss: 0.3667 - val_dense_1_loss: 0.4465 - val_dense_2_loss: 0.4259 - val_dense_3_loss: 0.3456 - val_dense_4_loss: 0.6350 - val_dense_5_loss: 0.1679 - val_dense_6_loss: 0.3366 - val_dense_7_loss: 0.7705 - val_dense_8_loss: 0.4848 - val_dense_9_loss: 0.7000 - val_dense_10_loss: 0.5737 - val_dense_11_loss: 0.5256 - val_dense_12_loss: 0.6560 - val_dense_13_loss: 0.5218 - val_dense_14_loss: 0.7368 - val_dense_15_loss: 0.6260 - val_dense_16_loss: 0.6512 - val_dense_17_loss: 0.3647 - val_dense_18_loss: 0.5447 - val_dense_19_loss: 0.4137 - val_dense_acc: 0.8802 - val_dense_1_acc: 0.8269 - val_dense_2_acc: 0.8590 - val_dense_3_acc: 0.8962 - val_dense_4_acc: 0.7632 - val_dense_5_acc: 0.9596 - val_dense_6_acc: 0.9025 - val_dense_7_acc: 0.7166 - val_dense_8_acc: 0.8356 - val_dense_9_acc: 0.7290 - val_dense_10_acc: 0.7806 - val_dense_11_acc: 0.8181 - val_dense_12_acc: 0.7632 - val_dense_13_acc: 0.8143 - val_dense_14_acc: 0.7352 - val_dense_15_acc: 0.7381 - val_dense_16_acc: 0.7589 - val_dense_17_acc: 0.8821 - val_dense_18_acc: 0.7952 - val_dense_19_acc: 0.8463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 10.14208\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n"
     ]
    }
   ],
   "source": [
    "model = build_model_2(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tok.texts_to_sequences(x_val)\n",
    "x_val =pad_sequences(x_val,maxlen=maxlen)\n",
    "test_pred = model.predict(x_test)\n",
    "val_pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5153724755221121\n",
      "0.4320045948252489\n",
      "0.5753894453335282\n",
      "0.48944740648228197\n",
      "0.6753658018472426\n",
      "0.5935173098040541\n",
      "0.5982643124291033\n",
      "0.6418858567136693\n",
      "0.5437121954143447\n",
      "0.5164455002615214\n",
      "0.5871087837156324\n",
      "0.6275878989205224\n",
      "0.6274261156695615\n",
      "0.6322647808780173\n",
      "0.5953753005399044\n",
      "0.6395808698064487\n",
      "0.3957022440424496\n",
      "0.6585192386249579\n",
      "0.5435483732746309\n",
      "0.6634370845725781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xq/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_list = []\n",
    "for pred, true in zip(val_pred, y_val):\n",
    "    F1 = f1_score(np.argmax(pred, axis=1), np.argmax(true, axis=1),average='macro')\n",
    "    print(F1)\n",
    "    f1_list.append(F1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5775977794338905"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_name, header=0, encoding=\"utf-8\"):\n",
    "\n",
    "    data_df = pd.read_csv(file_name, header=header, encoding=encoding)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data_from_csv(\"../inputs/sentiment_analysis_testa.csv\")\n",
    "for pred, column in zip(test_pred, columns):\n",
    "    test[column] = np.argmax(pred, axis=1) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>...</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"我想说他们家的优惠活动好持久啊，我预售的时候买的券，前两天心血来潮去吃的活动还在继续\\n首...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"终于开到心心念念的LAB loft。第一次来就随便点也一些～【香辣虾意面】蛮辣的，但其实一...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"地理位置好，交通方便，就在124车站对面交通方便，很好，我晚上7点多去买的了，已经没有什么...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"运气很好，抽中了大众点评的霸王餐。这家主题餐厅心仪已久了，种种原因一直未能成行，没想到抽中...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"幸运随点评团体验霸王餐，心情好~蜀九香刚进驻泉州不久，招牌大名气响，以至于刚到店门口的我被...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"尽管韩国烤肉店在无锡已经有很多家了，但因为味道好吃再加上在无锡很有人气，依旧挡不住新店的开...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\"店铺在乙烯生活二区的西北角，旁边是国旅的旗舰店，门口就是红绿灯，挺好找的。因为这是两磅一的...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\"朋友聚会来滴，这个地方真心不错哇，又能撸串，又能唱歌，还能看现场歌手演唱。最主要是可以上台...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>\"超喜欢这家的面食，经常来吃。这家风格及产品都很像京城御面，有时会怀疑是不是同一家的。面条比...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\"广西阳朔，前一天晚上吃的特色啤酒鱼，就看到了隔壁有家螺蛳粉，第二天过来吃。个人比较喜欢粉，...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>\"蛮早之前就在人气美食看过报道这家的夜宵了，一直想来看看，终于这次来了~~\\n大概晚上10点...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>\"中秋花好月圆之夜,一家老小共八人选择此地吃团圆饭。吴中店是新店，环境干净大气\\n整洁，提前...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>\"地址：他家位于万达广场C区，也就是说面向万达广场正面左边方向的后面商铺，要从背面商铺一楼寻...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>\"没错，我就是楼上的室友。已经告别自助三年的我曾经发誓再也不吃自助这种只有量没有质的东西。可...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>\"我亲爱的姐姐人品爆发抽中了豪华双人霸王餐，为了能吃上这一顿我还特意提前了一天从贵阳回到成都...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>\"＃试吃/体验点评＃首先还是照例感谢大众点评和商家抽中我！感谢好运气！这次想着带父皇母后来体...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\"原价比较贵，大众点评立减17的优惠很给力，可以点豪d，两个人原价213，折后149，吃得很...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\"原来去过一家浮力森林，就是铂金城的那家。\\n\\n但不知道从好久那家也不复存在。\\n\\n本来...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>\"首先要感谢福之源创意料理还有大众点评给予的这次机会，知道自己被抽中免费试吃的时候心情还是很...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>\"之前参与了抢购的活动，午餐和晚餐通用，每人只限一张，每张9.9元，吃烤肉自助真是便宜到家了...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>\"腾冲市区交通方便，周日晚饭上座率6成。\\n主打烤鸭和火锅。\\n本次没有点火锅，只吃了烤鸭和...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>\"本来和几个同学一起到光谷想吃东西的，可是到处都排队，然后大众搜了一下，刚好看到这家川菜，然...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>\"菜的口味偏咸，但是味道不错，量大实惠，就是服务员的态度实在不敢恭维，一副强卖的架势，非得让...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>\"去南汇玩的时候下午无聊中就想着出去吃甜品，老公的妹妹推荐的店，说是味道、环境还不错，于是我...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>\"中午过来吃饭，因为和单位很近。之前都是一直只吃碗面就好，今天来吃了次炒菜。酸菜鱼还是有点辣...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>\"这地方高峰时期都得等位，不全是因为好吃，有特色占了很大的比率。店里环境干净整洁，桌子上纸巾...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>\"分量很大。没有吃完打包了！位于大寨沟普通公交车车站附近，价格比较亲民。来过很多次。在两个套...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>\"安排了一天去银座逛街的行程，早上10点多就坐地铁过去了，早饭也没吃，想着到了银座可以早午饭...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>\"只能说这家店懂得网络营销，盲目的看大众点评真的是人云亦云，看点评来的，榆钱48，以为是本地...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>\"#昨晚去平江路压完马路经过哑巴生煎，看到旁边的这家粽子店，早有耳闻，一直想吃吃，店面挺大也...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14970</th>\n",
       "      <td>14970</td>\n",
       "      <td>\"环境：共两层，主要就餐区位于二楼，一楼是厨房。二层包括大厅还有蒙古包式的开放单间，还有小舞...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14971</th>\n",
       "      <td>14971</td>\n",
       "      <td>\"感谢点评抽中的试吃霸王餐\\n首先说哈位置，就在现在已经成熟火爆的九街，苏荷酒吧斜对面，吃货...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14972</th>\n",
       "      <td>14972</td>\n",
       "      <td>\"这家在长辈群体里口碑相当好\\n离家很近，爷爷奶奶有时候懒得做饭了会过来吃\\n性价比也很好，...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>14973</td>\n",
       "      <td>\"丁丁麻辣烫位于和义路的边上，招牌很引人关注，白色的底子，红色的丁丁麻辣烫五个大字很引人注目...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>14974</td>\n",
       "      <td>\"香格里拉，热情好客，带宾客如家人是香格里拉主打的招牌，但是在我看来唐山这家刚刚开业没有一两...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14975</th>\n",
       "      <td>14975</td>\n",
       "      <td>\"实在是吃不下中山三院饭堂那么难吃的伙食，于是就出去寻觅好吃的早餐，要知道一个好的早餐才能病...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>14976</td>\n",
       "      <td>\"简单的重庆小面不简单。\\n首先来说面。筋道，那是必须的，但是不硬，软Q，爽滑。量大，女孩子...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>14977</td>\n",
       "      <td>\"【环境】超有日本居酒屋的氛围～店铺不大。一共俩层。一层可以坐十二个人左右～位子有些紧。二层...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>14978</td>\n",
       "      <td>\"作为一个一个月内来了三次，一次裸蛋糕两次冰皮月饼的人。。自我感觉还是可以来说点什么的~~D...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>14979</td>\n",
       "      <td>\"同事推荐的吃早饭的地方，离婆婆家不远，但平时不会特地过来\\n唐山路公平路路口，车站旁边，不...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>14980</td>\n",
       "      <td>\"公司聚餐 听说这个是之前和平广场的那家 于是大家中午没吃饭就等这顿哈哈哈 去了之后果然没失...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14981</th>\n",
       "      <td>14981</td>\n",
       "      <td>\"就在步行街上，非常好找，酒店check in后就出来觅食了，9点多，店铺刚开张的感觉，好多...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>14982</td>\n",
       "      <td>\"在附近挑了好几家火锅店最后选中了这家～冲着专门吃牛肉火锅来的～总体感觉一般吧，都是按照招牌...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>14983</td>\n",
       "      <td>\"Enjoy上订购的双人套餐。整体来说，店内设计雅致，环境安静、食材新鲜，服务周到。\\n店门...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>14984</td>\n",
       "      <td>\"新城市广场，三楼，算是靠近电影院，大概20来米的样子\\n网上的团购，比较划算\\n首先，当时...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>14985</td>\n",
       "      <td>\"今天给个四星，比之前多一星，胜在服务上。照常点了招牌的外婆红烧肉，肥而不腻，肉很大一块，很...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>14986</td>\n",
       "      <td>\"娜娜家的装修真的是别具风格，就连烟灰缸都那么有特点，里面放的不是土，貌似是咖啡类的东东，味...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>14987</td>\n",
       "      <td>\"每次都是吃完才记得拍照也是醉了(/ω＼)\\n\\n这次消费体验一般吧～因为太早去了，5点就到...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14988</th>\n",
       "      <td>14988</td>\n",
       "      <td>\"昨天和小伙伴去吃的夜宵，话说，吃撑了一餐，再去的奇葩真的很少见。\\n对于海底捞，对我来说是...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>14989</td>\n",
       "      <td>\"首先很感谢大众点评给的这次试吃机会，在周日的晚上和众多小伙伴们美美地聚餐！\\n捉虾记 or...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>14990</td>\n",
       "      <td>\"888附近基本没有韩式冷面的店，好想吃，最近的也就是这家了。周五翘班过来吃。来的比较晚，都...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>14991</td>\n",
       "      <td>\"说这里性价比很高的人，一定是没有吃过六六寿司，强烈推荐六六寿司的定食套餐，绝对比这里吃的爽...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>14992</td>\n",
       "      <td>\"很幸运的中了霸王餐，上午考完试用脑过度刚好补充体力。位置还是很好找的，现在的烧烤店没点特色...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>14993</td>\n",
       "      <td>\"经常来吃的一家披萨店，珠江路和金鹰三期都有，来的金鹰的八楼，来的时候前面还有七桌，可以扫一...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>14994</td>\n",
       "      <td>\"真的不得不说，里面几个男服务员服务真的很好，很耐心！收盘子也很积极！说说菜的味道吧，总体来...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14995</td>\n",
       "      <td>\"杭州大厦周围的日料店有三上、山葵这样的连锁店，听说三上除了C座地下一楼的那家以外还要在D座...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14996</td>\n",
       "      <td>\"非常物美价廉的一家自助餐厅，虽是中午，之前担心的货少量不足的问题完全没出现，最爱的三文鱼、...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14997</td>\n",
       "      <td>\"生日的时候不知道去吃什么，偶然在大牌抢购里发现了这家店～感觉团购很给力，就果断团了一份～地...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14998</td>\n",
       "      <td>\"每次来大理都有不一样的感动，还记得上次来大理遇到一个特别好的出租车司机，这次来大理同样是这...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>14999</td>\n",
       "      <td>\"这家店位于城中湖码头，靠近广场非常近，离我们的住处比较近，开车过来只要几分钟的时间就好了。...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            content  \\\n",
       "0          0  \"我想说他们家的优惠活动好持久啊，我预售的时候买的券，前两天心血来潮去吃的活动还在继续\\n首...   \n",
       "1          1  \"终于开到心心念念的LAB loft。第一次来就随便点也一些～【香辣虾意面】蛮辣的，但其实一...   \n",
       "2          2  \"地理位置好，交通方便，就在124车站对面交通方便，很好，我晚上7点多去买的了，已经没有什么...   \n",
       "3          3  \"运气很好，抽中了大众点评的霸王餐。这家主题餐厅心仪已久了，种种原因一直未能成行，没想到抽中...   \n",
       "4          4  \"幸运随点评团体验霸王餐，心情好~蜀九香刚进驻泉州不久，招牌大名气响，以至于刚到店门口的我被...   \n",
       "5          5  \"尽管韩国烤肉店在无锡已经有很多家了，但因为味道好吃再加上在无锡很有人气，依旧挡不住新店的开...   \n",
       "6          6  \"店铺在乙烯生活二区的西北角，旁边是国旅的旗舰店，门口就是红绿灯，挺好找的。因为这是两磅一的...   \n",
       "7          7  \"朋友聚会来滴，这个地方真心不错哇，又能撸串，又能唱歌，还能看现场歌手演唱。最主要是可以上台...   \n",
       "8          8  \"超喜欢这家的面食，经常来吃。这家风格及产品都很像京城御面，有时会怀疑是不是同一家的。面条比...   \n",
       "9          9  \"广西阳朔，前一天晚上吃的特色啤酒鱼，就看到了隔壁有家螺蛳粉，第二天过来吃。个人比较喜欢粉，...   \n",
       "10        10  \"蛮早之前就在人气美食看过报道这家的夜宵了，一直想来看看，终于这次来了~~\\n大概晚上10点...   \n",
       "11        11  \"中秋花好月圆之夜,一家老小共八人选择此地吃团圆饭。吴中店是新店，环境干净大气\\n整洁，提前...   \n",
       "12        12  \"地址：他家位于万达广场C区，也就是说面向万达广场正面左边方向的后面商铺，要从背面商铺一楼寻...   \n",
       "13        13  \"没错，我就是楼上的室友。已经告别自助三年的我曾经发誓再也不吃自助这种只有量没有质的东西。可...   \n",
       "14        14  \"我亲爱的姐姐人品爆发抽中了豪华双人霸王餐，为了能吃上这一顿我还特意提前了一天从贵阳回到成都...   \n",
       "15        15  \"＃试吃/体验点评＃首先还是照例感谢大众点评和商家抽中我！感谢好运气！这次想着带父皇母后来体...   \n",
       "16        16  \"原价比较贵，大众点评立减17的优惠很给力，可以点豪d，两个人原价213，折后149，吃得很...   \n",
       "17        17  \"原来去过一家浮力森林，就是铂金城的那家。\\n\\n但不知道从好久那家也不复存在。\\n\\n本来...   \n",
       "18        18  \"首先要感谢福之源创意料理还有大众点评给予的这次机会，知道自己被抽中免费试吃的时候心情还是很...   \n",
       "19        19  \"之前参与了抢购的活动，午餐和晚餐通用，每人只限一张，每张9.9元，吃烤肉自助真是便宜到家了...   \n",
       "20        20  \"腾冲市区交通方便，周日晚饭上座率6成。\\n主打烤鸭和火锅。\\n本次没有点火锅，只吃了烤鸭和...   \n",
       "21        21  \"本来和几个同学一起到光谷想吃东西的，可是到处都排队，然后大众搜了一下，刚好看到这家川菜，然...   \n",
       "22        22  \"菜的口味偏咸，但是味道不错，量大实惠，就是服务员的态度实在不敢恭维，一副强卖的架势，非得让...   \n",
       "23        23  \"去南汇玩的时候下午无聊中就想着出去吃甜品，老公的妹妹推荐的店，说是味道、环境还不错，于是我...   \n",
       "24        24  \"中午过来吃饭，因为和单位很近。之前都是一直只吃碗面就好，今天来吃了次炒菜。酸菜鱼还是有点辣...   \n",
       "25        25  \"这地方高峰时期都得等位，不全是因为好吃，有特色占了很大的比率。店里环境干净整洁，桌子上纸巾...   \n",
       "26        26  \"分量很大。没有吃完打包了！位于大寨沟普通公交车车站附近，价格比较亲民。来过很多次。在两个套...   \n",
       "27        27  \"安排了一天去银座逛街的行程，早上10点多就坐地铁过去了，早饭也没吃，想着到了银座可以早午饭...   \n",
       "28        28  \"只能说这家店懂得网络营销，盲目的看大众点评真的是人云亦云，看点评来的，榆钱48，以为是本地...   \n",
       "29        29  \"#昨晚去平江路压完马路经过哑巴生煎，看到旁边的这家粽子店，早有耳闻，一直想吃吃，店面挺大也...   \n",
       "...      ...                                                ...   \n",
       "14970  14970  \"环境：共两层，主要就餐区位于二楼，一楼是厨房。二层包括大厅还有蒙古包式的开放单间，还有小舞...   \n",
       "14971  14971  \"感谢点评抽中的试吃霸王餐\\n首先说哈位置，就在现在已经成熟火爆的九街，苏荷酒吧斜对面，吃货...   \n",
       "14972  14972  \"这家在长辈群体里口碑相当好\\n离家很近，爷爷奶奶有时候懒得做饭了会过来吃\\n性价比也很好，...   \n",
       "14973  14973  \"丁丁麻辣烫位于和义路的边上，招牌很引人关注，白色的底子，红色的丁丁麻辣烫五个大字很引人注目...   \n",
       "14974  14974  \"香格里拉，热情好客，带宾客如家人是香格里拉主打的招牌，但是在我看来唐山这家刚刚开业没有一两...   \n",
       "14975  14975  \"实在是吃不下中山三院饭堂那么难吃的伙食，于是就出去寻觅好吃的早餐，要知道一个好的早餐才能病...   \n",
       "14976  14976  \"简单的重庆小面不简单。\\n首先来说面。筋道，那是必须的，但是不硬，软Q，爽滑。量大，女孩子...   \n",
       "14977  14977  \"【环境】超有日本居酒屋的氛围～店铺不大。一共俩层。一层可以坐十二个人左右～位子有些紧。二层...   \n",
       "14978  14978  \"作为一个一个月内来了三次，一次裸蛋糕两次冰皮月饼的人。。自我感觉还是可以来说点什么的~~D...   \n",
       "14979  14979  \"同事推荐的吃早饭的地方，离婆婆家不远，但平时不会特地过来\\n唐山路公平路路口，车站旁边，不...   \n",
       "14980  14980  \"公司聚餐 听说这个是之前和平广场的那家 于是大家中午没吃饭就等这顿哈哈哈 去了之后果然没失...   \n",
       "14981  14981  \"就在步行街上，非常好找，酒店check in后就出来觅食了，9点多，店铺刚开张的感觉，好多...   \n",
       "14982  14982  \"在附近挑了好几家火锅店最后选中了这家～冲着专门吃牛肉火锅来的～总体感觉一般吧，都是按照招牌...   \n",
       "14983  14983  \"Enjoy上订购的双人套餐。整体来说，店内设计雅致，环境安静、食材新鲜，服务周到。\\n店门...   \n",
       "14984  14984  \"新城市广场，三楼，算是靠近电影院，大概20来米的样子\\n网上的团购，比较划算\\n首先，当时...   \n",
       "14985  14985  \"今天给个四星，比之前多一星，胜在服务上。照常点了招牌的外婆红烧肉，肥而不腻，肉很大一块，很...   \n",
       "14986  14986  \"娜娜家的装修真的是别具风格，就连烟灰缸都那么有特点，里面放的不是土，貌似是咖啡类的东东，味...   \n",
       "14987  14987  \"每次都是吃完才记得拍照也是醉了(/ω＼)\\n\\n这次消费体验一般吧～因为太早去了，5点就到...   \n",
       "14988  14988  \"昨天和小伙伴去吃的夜宵，话说，吃撑了一餐，再去的奇葩真的很少见。\\n对于海底捞，对我来说是...   \n",
       "14989  14989  \"首先很感谢大众点评给的这次试吃机会，在周日的晚上和众多小伙伴们美美地聚餐！\\n捉虾记 or...   \n",
       "14990  14990  \"888附近基本没有韩式冷面的店，好想吃，最近的也就是这家了。周五翘班过来吃。来的比较晚，都...   \n",
       "14991  14991  \"说这里性价比很高的人，一定是没有吃过六六寿司，强烈推荐六六寿司的定食套餐，绝对比这里吃的爽...   \n",
       "14992  14992  \"很幸运的中了霸王餐，上午考完试用脑过度刚好补充体力。位置还是很好找的，现在的烧烤店没点特色...   \n",
       "14993  14993  \"经常来吃的一家披萨店，珠江路和金鹰三期都有，来的金鹰的八楼，来的时候前面还有七桌，可以扫一...   \n",
       "14994  14994  \"真的不得不说，里面几个男服务员服务真的很好，很耐心！收盘子也很积极！说说菜的味道吧，总体来...   \n",
       "14995  14995  \"杭州大厦周围的日料店有三上、山葵这样的连锁店，听说三上除了C座地下一楼的那家以外还要在D座...   \n",
       "14996  14996  \"非常物美价廉的一家自助餐厅，虽是中午，之前担心的货少量不足的问题完全没出现，最爱的三文鱼、...   \n",
       "14997  14997  \"生日的时候不知道去吃什么，偶然在大牌抢购里发现了这家店～感觉团购很给力，就果断团了一份～地...   \n",
       "14998  14998  \"每次来大理都有不一样的感动，还记得上次来大理遇到一个特别好的出租车司机，这次来大理同样是这...   \n",
       "14999  14999  \"这家店位于城中湖码头，靠近广场非常近，离我们的住处比较近，开车过来只要几分钟的时间就好了。...   \n",
       "\n",
       "       location_traffic_convenience  location_distance_from_business_district  \\\n",
       "0                                -2                                        -2   \n",
       "1                                -2                                        -2   \n",
       "2                                 1                                         1   \n",
       "3                                 1                                         1   \n",
       "4                                -2                                        -2   \n",
       "5                                -2                                        -2   \n",
       "6                                -2                                        -2   \n",
       "7                                -2                                        -2   \n",
       "8                                -2                                        -2   \n",
       "9                                -2                                        -2   \n",
       "10                               -2                                        -2   \n",
       "11                               -2                                        -2   \n",
       "12                               -2                                         1   \n",
       "13                               -2                                        -2   \n",
       "14                               -2                                        -2   \n",
       "15                               -2                                        -2   \n",
       "16                               -2                                        -2   \n",
       "17                               -2                                        -2   \n",
       "18                               -2                                        -2   \n",
       "19                               -2                                        -2   \n",
       "20                               -2                                        -2   \n",
       "21                               -2                                        -2   \n",
       "22                               -2                                        -2   \n",
       "23                               -2                                        -2   \n",
       "24                               -2                                        -2   \n",
       "25                               -2                                        -2   \n",
       "26                                1                                        -2   \n",
       "27                                1                                        -2   \n",
       "28                               -2                                        -2   \n",
       "29                               -2                                        -2   \n",
       "...                             ...                                       ...   \n",
       "14970                            -2                                        -2   \n",
       "14971                            -2                                        -2   \n",
       "14972                            -2                                        -2   \n",
       "14973                            -2                                        -2   \n",
       "14974                            -2                                        -2   \n",
       "14975                            -2                                        -2   \n",
       "14976                            -2                                        -2   \n",
       "14977                             1                                        -2   \n",
       "14978                            -2                                        -2   \n",
       "14979                             1                                        -2   \n",
       "14980                            -2                                         1   \n",
       "14981                            -2                                        -2   \n",
       "14982                            -2                                        -2   \n",
       "14983                            -2                                        -2   \n",
       "14984                            -2                                         1   \n",
       "14985                            -2                                        -2   \n",
       "14986                            -2                                        -2   \n",
       "14987                            -2                                        -2   \n",
       "14988                            -2                                        -2   \n",
       "14989                            -2                                        -2   \n",
       "14990                            -2                                        -2   \n",
       "14991                            -2                                        -2   \n",
       "14992                            -2                                        -2   \n",
       "14993                            -2                                        -2   \n",
       "14994                            -2                                        -2   \n",
       "14995                            -2                                        -2   \n",
       "14996                            -2                                        -2   \n",
       "14997                            -2                                        -2   \n",
       "14998                            -2                                        -2   \n",
       "14999                             1                                         1   \n",
       "\n",
       "       location_easy_to_find  service_wait_time  service_waiters_attitude  \\\n",
       "0                         -2                 -2                         1   \n",
       "1                         -2                 -2                        -2   \n",
       "2                          1                 -2                        -1   \n",
       "3                         -2                 -2                         1   \n",
       "4                         -2                 -2                         1   \n",
       "5                         -2                 -2                         1   \n",
       "6                          1                 -2                         1   \n",
       "7                         -2                 -2                        -2   \n",
       "8                         -2                 -2                        -2   \n",
       "9                          1                 -2                        -2   \n",
       "10                        -2                 -2                         1   \n",
       "11                        -2                 -2                         1   \n",
       "12                        -1                 -2                        -2   \n",
       "13                        -2                 -1                        -1   \n",
       "14                        -2                 -2                        -2   \n",
       "15                        -2                 -2                         1   \n",
       "16                        -2                 -2                         1   \n",
       "17                        -2                 -2                        -2   \n",
       "18                        -2                 -2                        -2   \n",
       "19                        -2                 -2                        -2   \n",
       "20                        -2                 -2                        -1   \n",
       "21                        -2                 -2                         1   \n",
       "22                        -2                 -2                        -1   \n",
       "23                        -2                 -2                         0   \n",
       "24                        -2                 -2                        -2   \n",
       "25                        -2                 -2                        -2   \n",
       "26                        -2                 -2                        -2   \n",
       "27                        -2                 -2                         0   \n",
       "28                        -2                 -2                        -2   \n",
       "29                        -2                 -2                        -2   \n",
       "...                      ...                ...                       ...   \n",
       "14970                     -2                 -2                        -2   \n",
       "14971                     -2                 -2                        -2   \n",
       "14972                     -2                 -2                        -2   \n",
       "14973                     -2                  0                        -2   \n",
       "14974                     -2                 -2                        -1   \n",
       "14975                     -2                 -2                        -2   \n",
       "14976                     -2                 -2                         1   \n",
       "14977                     -1                 -2                         1   \n",
       "14978                     -2                 -2                         1   \n",
       "14979                     -2                 -2                        -2   \n",
       "14980                     -2                 -2                         1   \n",
       "14981                      1                 -2                         1   \n",
       "14982                     -2                 -2                         1   \n",
       "14983                     -2                 -2                        -2   \n",
       "14984                     -2                 -2                         1   \n",
       "14985                     -2                 -2                         1   \n",
       "14986                     -2                 -2                        -2   \n",
       "14987                     -2                 -2                         1   \n",
       "14988                     -2                 -2                        -2   \n",
       "14989                     -2                 -2                        -2   \n",
       "14990                     -2                 -2                        -2   \n",
       "14991                     -2                 -2                        -2   \n",
       "14992                      1                 -2                         1   \n",
       "14993                     -2                 -2                         0   \n",
       "14994                     -2                 -2                        -1   \n",
       "14995                     -2                 -2                        -2   \n",
       "14996                     -2                 -2                         1   \n",
       "14997                      1                 -2                         1   \n",
       "14998                     -2                 -2                         1   \n",
       "14999                     -2                 -2                        -2   \n",
       "\n",
       "       service_parking_convenience  service_serving_speed  price_level  \\\n",
       "0                               -2                     -2            0   \n",
       "1                               -2                     -2            0   \n",
       "2                               -2                     -2            0   \n",
       "3                               -2                     -2           -2   \n",
       "4                               -2                     -2           -2   \n",
       "5                               -2                     -2           -1   \n",
       "6                               -2                     -2            1   \n",
       "7                               -2                     -2           -2   \n",
       "8                               -2                     -2            0   \n",
       "9                                1                      1            0   \n",
       "10                              -2                     -2           -2   \n",
       "11                              -2                     -2           -2   \n",
       "12                              -2                     -2           -1   \n",
       "13                              -2                     -2           -2   \n",
       "14                              -2                     -2           -2   \n",
       "15                              -2                     -2           -2   \n",
       "16                              -2                     -2           -1   \n",
       "17                              -2                     -2            0   \n",
       "18                              -2                     -2            1   \n",
       "19                              -2                      1            1   \n",
       "20                              -2                     -2           -2   \n",
       "21                              -2                     -2           -2   \n",
       "22                              -2                     -1           -2   \n",
       "23                              -2                     -2           -2   \n",
       "24                              -2                     -2            1   \n",
       "25                              -2                      0           -2   \n",
       "26                              -2                     -2            1   \n",
       "27                              -2                     -2           -2   \n",
       "28                              -2                     -2            0   \n",
       "29                              -2                     -2            0   \n",
       "...                            ...                    ...          ...   \n",
       "14970                           -2                     -2           -1   \n",
       "14971                           -2                     -2           -2   \n",
       "14972                           -1                     -2           -1   \n",
       "14973                           -2                     -2           -2   \n",
       "14974                           -2                     -2            1   \n",
       "14975                           -2                     -2            1   \n",
       "14976                           -2                      1            1   \n",
       "14977                           -2                     -2           -2   \n",
       "14978                           -2                     -2           -2   \n",
       "14979                           -2                     -2           -2   \n",
       "14980                           -2                     -2           -2   \n",
       "14981                           -2                     -2           -2   \n",
       "14982                           -2                     -2            0   \n",
       "14983                            1                     -2           -2   \n",
       "14984                           -2                      1           -2   \n",
       "14985                           -2                     -2           -2   \n",
       "14986                           -2                     -2           -2   \n",
       "14987                           -2                     -2            0   \n",
       "14988                           -2                     -2           -1   \n",
       "14989                           -2                     -2            0   \n",
       "14990                           -2                      1           -1   \n",
       "14991                           -2                     -2           -1   \n",
       "14992                           -2                     -2           -2   \n",
       "14993                           -2                     -2           -2   \n",
       "14994                           -2                     -2            0   \n",
       "14995                           -2                      1            1   \n",
       "14996                           -2                     -2           -2   \n",
       "14997                           -2                     -2           -2   \n",
       "14998                           -2                     -2           -2   \n",
       "14999                           -2                     -2           -2   \n",
       "\n",
       "                    ...                 environment_decoration  \\\n",
       "0                   ...                                     -2   \n",
       "1                   ...                                     -2   \n",
       "2                   ...                                     -2   \n",
       "3                   ...                                      1   \n",
       "4                   ...                                      1   \n",
       "5                   ...                                     -2   \n",
       "6                   ...                                     -2   \n",
       "7                   ...                                      1   \n",
       "8                   ...                                     -2   \n",
       "9                   ...                                     -2   \n",
       "10                  ...                                     -2   \n",
       "11                  ...                                      1   \n",
       "12                  ...                                      0   \n",
       "13                  ...                                     -2   \n",
       "14                  ...                                     -2   \n",
       "15                  ...                                      1   \n",
       "16                  ...                                     -2   \n",
       "17                  ...                                     -2   \n",
       "18                  ...                                     -2   \n",
       "19                  ...                                     -2   \n",
       "20                  ...                                     -2   \n",
       "21                  ...                                      1   \n",
       "22                  ...                                     -2   \n",
       "23                  ...                                      1   \n",
       "24                  ...                                     -2   \n",
       "25                  ...                                     -2   \n",
       "26                  ...                                     -2   \n",
       "27                  ...                                     -2   \n",
       "28                  ...                                     -2   \n",
       "29                  ...                                     -2   \n",
       "...                 ...                                    ...   \n",
       "14970               ...                                     -2   \n",
       "14971               ...                                     -2   \n",
       "14972               ...                                      0   \n",
       "14973               ...                                     -2   \n",
       "14974               ...                                     -2   \n",
       "14975               ...                                     -2   \n",
       "14976               ...                                      1   \n",
       "14977               ...                                     -2   \n",
       "14978               ...                                      1   \n",
       "14979               ...                                     -2   \n",
       "14980               ...                                      1   \n",
       "14981               ...                                     -2   \n",
       "14982               ...                                     -2   \n",
       "14983               ...                                      1   \n",
       "14984               ...                                     -2   \n",
       "14985               ...                                     -2   \n",
       "14986               ...                                      1   \n",
       "14987               ...                                     -2   \n",
       "14988               ...                                      1   \n",
       "14989               ...                                     -2   \n",
       "14990               ...                                     -2   \n",
       "14991               ...                                      0   \n",
       "14992               ...                                      1   \n",
       "14993               ...                                     -2   \n",
       "14994               ...                                     -2   \n",
       "14995               ...                                      0   \n",
       "14996               ...                                     -2   \n",
       "14997               ...                                     -2   \n",
       "14998               ...                                     -2   \n",
       "14999               ...                                     -2   \n",
       "\n",
       "       environment_noise  environment_space  environment_cleaness  \\\n",
       "0                     -2                 -2                     1   \n",
       "1                     -2                 -2                    -2   \n",
       "2                     -2                 -2                    -2   \n",
       "3                     -2                 -2                    -2   \n",
       "4                     -2                 -2                    -2   \n",
       "5                     -2                 -2                    -2   \n",
       "6                     -2                 -2                    -2   \n",
       "7                      1                 -2                    -2   \n",
       "8                     -2                 -2                    -2   \n",
       "9                     -2                 -2                     1   \n",
       "10                    -2                 -2                     1   \n",
       "11                    -2                 -2                    -2   \n",
       "12                    -2                  0                    -2   \n",
       "13                    -2                 -2                    -2   \n",
       "14                    -2                 -2                    -2   \n",
       "15                     1                  1                     1   \n",
       "16                    -2                 -2                    -2   \n",
       "17                    -2                 -2                    -2   \n",
       "18                    -2                 -2                    -2   \n",
       "19                    -2                 -2                    -2   \n",
       "20                    -2                 -2                    -2   \n",
       "21                    -2                 -2                    -2   \n",
       "22                    -2                 -2                    -2   \n",
       "23                     1                  1                     1   \n",
       "24                    -2                 -2                    -2   \n",
       "25                    -2                 -2                     1   \n",
       "26                    -2                 -2                    -2   \n",
       "27                    -2                 -2                    -2   \n",
       "28                    -2                 -2                    -2   \n",
       "29                    -2                 -2                     1   \n",
       "...                  ...                ...                   ...   \n",
       "14970                 -2                 -2                    -2   \n",
       "14971                 -2                 -2                    -2   \n",
       "14972                  0                  0                     0   \n",
       "14973                 -2                 -2                    -2   \n",
       "14974                 -2                 -2                    -2   \n",
       "14975                 -2                 -2                    -2   \n",
       "14976                 -2                 -2                     1   \n",
       "14977                 -2                  0                    -2   \n",
       "14978                 -2                 -2                    -2   \n",
       "14979                 -2                  0                    -2   \n",
       "14980                 -2                 -2                    -2   \n",
       "14981                 -2                 -2                    -2   \n",
       "14982                 -2                 -2                    -2   \n",
       "14983                  1                 -2                    -2   \n",
       "14984                 -2                 -2                    -2   \n",
       "14985                 -2                 -2                    -2   \n",
       "14986                 -2                 -2                    -2   \n",
       "14987                 -2                 -2                    -2   \n",
       "14988                 -2                 -2                     1   \n",
       "14989                 -2                 -2                    -2   \n",
       "14990                 -2                 -2                    -2   \n",
       "14991                  0                 -1                    -1   \n",
       "14992                  1                  1                     1   \n",
       "14993                 -2                 -2                    -2   \n",
       "14994                 -2                 -2                    -2   \n",
       "14995                 -2                 -2                    -2   \n",
       "14996                 -2                 -2                    -2   \n",
       "14997                 -2                 -2                    -2   \n",
       "14998                 -2                 -2                    -2   \n",
       "14999                 -2                 -2                    -2   \n",
       "\n",
       "       dish_portion  dish_taste  dish_look  dish_recommendation  \\\n",
       "0                 1           1         -2                   -2   \n",
       "1                -1           1         -2                   -2   \n",
       "2                -2           1         -2                   -2   \n",
       "3                -2           1         -2                    1   \n",
       "4                -2           1         -2                   -2   \n",
       "5                 0           1         -2                    1   \n",
       "6                 1           1         -2                   -2   \n",
       "7                -2           1         -2                   -2   \n",
       "8                 1           0         -2                   -2   \n",
       "9                -2           0         -2                   -2   \n",
       "10               -2           0         -2                   -2   \n",
       "11                1           1         -2                   -2   \n",
       "12               -2           0         -2                   -2   \n",
       "13               -2          -2         -2                   -2   \n",
       "14                1           1         -2                    1   \n",
       "15                1           1         -2                    1   \n",
       "16               -2           1         -2                   -2   \n",
       "17               -2           0         -2                   -2   \n",
       "18                1           1          1                   -2   \n",
       "19               -2           0         -2                   -2   \n",
       "20               -2           0         -2                   -2   \n",
       "21               -2           0         -2                   -2   \n",
       "22                1           1         -2                    1   \n",
       "23               -2           1         -2                   -2   \n",
       "24                0           0         -2                   -2   \n",
       "25                1           1         -2                   -2   \n",
       "26                1           1         -2                    1   \n",
       "27               -2           1         -2                   -2   \n",
       "28               -2          -1         -2                   -2   \n",
       "29                1           1         -2                   -2   \n",
       "...             ...         ...        ...                  ...   \n",
       "14970             1           0         -2                    0   \n",
       "14971            -2           0         -2                   -2   \n",
       "14972             1           1         -2                   -2   \n",
       "14973            -2           1         -2                   -2   \n",
       "14974            -1           0         -2                   -2   \n",
       "14975            -2           0         -2                   -2   \n",
       "14976             1           1         -2                   -2   \n",
       "14977             1           1         -2                   -2   \n",
       "14978            -2          -2         -2                   -2   \n",
       "14979             1           1         -2                   -2   \n",
       "14980            -2           1         -2                   -2   \n",
       "14981            -2           0         -2                   -2   \n",
       "14982            -2           0         -2                   -2   \n",
       "14983             1           1         -2                   -2   \n",
       "14984            -2           1         -2                   -2   \n",
       "14985             1           1         -2                   -2   \n",
       "14986            -2           1          1                   -2   \n",
       "14987            -2           1         -2                   -2   \n",
       "14988             1           1          1                   -2   \n",
       "14989             1           0         -2                   -1   \n",
       "14990            -2           0         -2                   -2   \n",
       "14991            -2          -2         -2                   -2   \n",
       "14992            -2           1         -2                   -2   \n",
       "14993             1           1         -2                   -2   \n",
       "14994            -2           1         -2                   -2   \n",
       "14995            -2          -2         -2                   -2   \n",
       "14996             1           1         -2                   -2   \n",
       "14997            -2           0         -2                   -2   \n",
       "14998            -2           1         -2                   -2   \n",
       "14999            -2           1         -2                   -2   \n",
       "\n",
       "       others_overall_experience  others_willing_to_consume_again  \n",
       "0                              1                                1  \n",
       "1                              1                                1  \n",
       "2                              0                               -2  \n",
       "3                              1                                1  \n",
       "4                              1                               -2  \n",
       "5                              1                               -2  \n",
       "6                              1                               -2  \n",
       "7                              1                                1  \n",
       "8                              0                               -2  \n",
       "9                              1                               -2  \n",
       "10                             1                                1  \n",
       "11                             1                               -2  \n",
       "12                             0                               -2  \n",
       "13                            -1                               -2  \n",
       "14                             1                                1  \n",
       "15                             1                                1  \n",
       "16                             1                               -2  \n",
       "17                             0                               -2  \n",
       "18                             1                                1  \n",
       "19                             1                               -2  \n",
       "20                            -1                               -1  \n",
       "21                             1                               -2  \n",
       "22                             1                               -2  \n",
       "23                             0                               -2  \n",
       "24                             1                               -2  \n",
       "25                             1                               -2  \n",
       "26                             1                                1  \n",
       "27                             1                               -2  \n",
       "28                            -1                                1  \n",
       "29                             1                                1  \n",
       "...                          ...                              ...  \n",
       "14970                          0                               -2  \n",
       "14971                          1                               -2  \n",
       "14972                          1                               -2  \n",
       "14973                          1                               -2  \n",
       "14974                         -1                               -2  \n",
       "14975                          1                                1  \n",
       "14976                          1                                1  \n",
       "14977                          1                               -2  \n",
       "14978                          1                                1  \n",
       "14979                          1                                1  \n",
       "14980                          1                                1  \n",
       "14981                          1                                1  \n",
       "14982                          0                               -2  \n",
       "14983                          1                               -2  \n",
       "14984                          1                                1  \n",
       "14985                          0                               -2  \n",
       "14986                          0                               -2  \n",
       "14987                          1                               -2  \n",
       "14988                          1                               -2  \n",
       "14989                          1                               -2  \n",
       "14990                          0                               -2  \n",
       "14991                         -1                               -2  \n",
       "14992                          1                                1  \n",
       "14993                          1                               -2  \n",
       "14994                         -1                               -2  \n",
       "14995                          1                               -2  \n",
       "14996                          1                                1  \n",
       "14997                          0                               -2  \n",
       "14998                          1                                1  \n",
       "14999                          1                                1  \n",
       "\n",
       "[15000 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"../output/bigru-cnn-pooling3.csv\", encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
