{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理, 数据保存到npz中\n",
    "\n",
    "1. train, vali, testa 的词id np array\n",
    "2. train, vali, test 的标签保存为 np array\n",
    "3. word2id保存为json\n",
    "4. embedding 保存为 np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "EMBEDDING_FILE = '../inputs/fasttextwordvec.vec'\n",
    "train = pd.read_csv(\"../inputs/train.csv\")\n",
    "test = pd.read_csv(\"../inputs/testa.csv\")\n",
    "val = pd.read_csv(\"../inputs/vali.csv\")\n",
    "X_train = train[\"content\"].fillna(\"无\").str.lower()\n",
    "X_val = val[\"content\"].fillna(\"无\").str.lower()\n",
    "X_test = test[\"content\"].fillna(\"无\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = pickle.load(open(\"../inputs/word2id.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据参数\n",
    "max_features=65462\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "word_index = word2id\n",
    "#prepare embedding matrix\n",
    "num_words = max_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.randn(num_words, embed_size) * 0.01\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存embedding\n",
    "np.savez_compressed(\"../inputs/fasttextwordvec.npz\",vector=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from typing import List\n",
    "\n",
    "columns = train.columns.tolist()[2:]\n",
    "def get_y(df:pd.DataFrame, cols:List[str]=columns) -> List[np.array]:\n",
    "    y_dict = dict()\n",
    "    for col in cols:\n",
    "        y = df[col].values + 2\n",
    "        y_ = to_categorical(y, num_classes=4)\n",
    "        y_dict[col] = y_\n",
    "    return y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_y(train)\n",
    "y_val = get_y(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../inputs/Y_train.npz\",**y_train)\n",
    "np.savez_compressed(\"../inputs/Y_valid.npz\",**y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(columns, open(\"../inputs/columns.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def transform_data_to_id(x_arr, word2id):\n",
    "    data = []\n",
    "\n",
    "    def map_word_to_id(word):\n",
    "        output = []\n",
    "        if word in word2id:\n",
    "            output.append(word2id[word])\n",
    "        else:\n",
    "            chars = list(word)\n",
    "            for char in chars:\n",
    "                if char in word2id:\n",
    "                    output.append(word2id[char])\n",
    "                else:\n",
    "                    output.append(1)\n",
    "        return output\n",
    "\n",
    "    def map_sent_to_id(sent):\n",
    "        output = []\n",
    "        for word in sent:\n",
    "            output.extend(map_word_to_id(word))\n",
    "        return output\n",
    "    for s in x_arr:\n",
    "        data.append(map_sent_to_id(s))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = transform_data_to_id(X_val, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "pad_sequence = keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = transform_data_to_id(X_val, word2id)\n",
    "X_val = pad_sequence(X_val,maxlen=maxlen,padding='pre',truncating='pre',value = 0)\n",
    "X_train = transform_data_to_id(X_train, word2id)\n",
    "X_train = pad_sequence(X_train,maxlen=maxlen,padding='pre',truncating='pre',value = 0)\n",
    "X_test = transform_data_to_id(X_test, word2id)\n",
    "X_test = pad_sequence(X_test,maxlen=maxlen,padding='pre',truncating='pre',value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../inputs/X_train.npz\",X = X_train)\n",
    "np.savez_compressed(\"../inputs/X_valid.npz\",X = X_val)\n",
    "np.savez_compressed(\"../inputs/X_test.npz\",X = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location_traffic_convenience',\n",
       " 'location_distance_from_business_district',\n",
       " 'location_easy_to_find',\n",
       " 'service_wait_time',\n",
       " 'service_waiters_attitude',\n",
       " 'service_parking_convenience',\n",
       " 'service_serving_speed',\n",
       " 'price_level',\n",
       " 'price_cost_effective',\n",
       " 'price_discount',\n",
       " 'environment_decoration',\n",
       " 'environment_noise',\n",
       " 'environment_space',\n",
       " 'environment_cleaness',\n",
       " 'dish_portion',\n",
       " 'dish_taste',\n",
       " 'dish_look',\n",
       " 'dish_recommendation',\n",
       " 'others_overall_experience',\n",
       " 'others_willing_to_consume_again']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "# train = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "# val = torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch as t\n",
    "import time\n",
    "\n",
    "class BasicModule(t.nn.Module):\n",
    "    '''\n",
    "    封装了nn.Module,主要是提供了save和load两个方法\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicModule,self).__init__()\n",
    "        self.model_name=str(type(self))# 默认名字\n",
    "\n",
    "    def load(self, path,change_opt=True):\n",
    "        print(path)\n",
    "        data = t.load(path)\n",
    "        if 'opt' in data:\n",
    "            # old_opt_stats = self.opt.state_dict() \n",
    "            if change_opt:\n",
    "                \n",
    "                self.opt.parse(data['opt'],print_=False)\n",
    "                self.opt.embedding_path=None\n",
    "                self.__init__(self.opt)\n",
    "            # self.opt.parse(old_opt_stats,print_=False)\n",
    "            self.load_state_dict(data['d'])\n",
    "        else:\n",
    "            self.load_state_dict(data)\n",
    "        return self.cuda()\n",
    "\n",
    "    def save(self, name=None,new=False):\n",
    "        prefix = '../ckpt/' + self.model_name + '_' +self.opt.type_+'_'\n",
    "        if name is None:\n",
    "            name = time.strftime('%m%d_%H:%M:%S.pth')\n",
    "        path = prefix+name\n",
    "\n",
    "        if new:\n",
    "            data = {'opt':self.opt.state_dict(),'d':self.state_dict()}\n",
    "        else:\n",
    "            data=self.state_dict()\n",
    "\n",
    "        t.save(data, path)\n",
    "        return path\n",
    "\n",
    "    def get_optimizer(self,lr1,lr2=0,weight_decay = 0):\n",
    "        ignored_params = list(map(id, self.encoder.parameters()))\n",
    "        base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "                        self.parameters())\n",
    "        if lr2 is None: lr2 = lr1*0.5 \n",
    "        optimizer = t.optim.Adam([\n",
    "                dict(params=base_params,weight_decay = weight_decay,lr=lr1),\n",
    "                {'params': self.encoder.parameters(), 'lr': lr2}\n",
    "            ])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self,cin,co,relu=True,norm=True):\n",
    "        super(Inception, self).__init__()\n",
    "        assert(co%4==0)\n",
    "        cos=[co//4]*4\n",
    "        self.activa=nn.Sequential()\n",
    "        if norm:self.activa.add_module('norm',nn.BatchNorm1d(co))\n",
    "        if relu:self.activa.add_module('relu',nn.ReLU(True))\n",
    "        self.branch1 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[0], 1,stride=1)),\n",
    "            ])) \n",
    "        self.branch2 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[1], 1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[1])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "        self.branch3 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[2], 3,padding=1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[2])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n",
    "            ]))\n",
    "        self.branch4 =nn.Sequential(OrderedDict([\n",
    "            #('pool',nn.MaxPool1d(2)),\n",
    "            ('conv3', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "    def forward(self,x):\n",
    "        branch1=self.branch1(x)\n",
    "        branch2=self.branch2(x)\n",
    "        branch3=self.branch3(x)\n",
    "        branch4=self.branch4(x)\n",
    "        result=self.activa(torch.cat((branch1,branch2,branch3,branch4),1))\n",
    "        return result\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle as pkl\n",
    "class CNNText_inception(BasicModule):\n",
    "    def __init__(self, opt):\n",
    "        super(CNNText_inception, self).__init__()\n",
    "        incept_dim=opt.inception_dim\n",
    "        self.label_list = pkl.load(open(opt.label_list_pkl, \"rb\"))\n",
    "        self.model_name = 'CNNText_inception'\n",
    "        self.opt=opt\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "        self.content_conv=nn.Sequential(\n",
    "            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)\n",
    "            #Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)\n",
    "            Inception(incept_dim,incept_dim),\n",
    "            nn.MaxPool1d(opt.content_seq_len)\n",
    "        )\n",
    "        self.fc_sub = nn.Sequential(\n",
    "            nn.Linear(incept_dim,opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n",
    "        )\n",
    "        self.fc = nn.ModuleDict({label: copy.deepcopy(self.fc_sub) for label in self.label_list}\n",
    "        )\n",
    "        if opt.embedding_path:\n",
    "            print('load embedding')\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        content_out=self.content_conv(content.permute(0,2,1))        \n",
    "        out = content_out.view(content_out.size(0), -1)\n",
    "        out_dict = {label: self.fc[label](out) for label in self.label_list}\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf8\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "class ModelConfig(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "    model='CNNText' \n",
    "    content_dim = 200 #描述的卷积核数\n",
    "    num_classes = 80 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 4 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2# k\n",
    "    hidden_size = 256 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 512 #inception的卷积核数\n",
    "    \n",
    "    vocab_size = 65462\n",
    "    kernel_size = 3 #单尺度卷积核\n",
    "    kernel_sizes = [2,3,4] #多尺度卷积核\n",
    "    content_seq_len = 200 \n",
    "    label_list_pkl = \"../inputs/columns.pkl\"\n",
    "    embedding_path = '../inputs/fasttextwordvec.npz' # Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelopt = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load embedding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = CNNText_inception(modelopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class FGSentimetDataset(data.Dataset):\n",
    "    def __init__(self, X_npz, Y_npz, label_pkl, augument=False, training=True):\n",
    "        self.augument=augument\n",
    "        self.training = training\n",
    "        self.label_list = pickle.load(open(label_pkl, 'rb'))\n",
    "        self.X = np.load(X_npz)['X']\n",
    "        self.Y = np.load(Y_npz)\n",
    "        self._len = self.X.shape[0]\n",
    "    def shuffle(self,d):\n",
    "        return np.random.permutation(d.tolist())\n",
    "\n",
    "    def dropout(self,d,p=0.5):\n",
    "        len_ = len(d)\n",
    "        index = np.random.choice(len_,int(len_*p))\n",
    "        d[index]=0\n",
    "        return d     \n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        content =  self.X[index]\n",
    "    \n",
    "        if self.training:  \n",
    "            if self.augument :\n",
    "                augument=random.random()\n",
    "\n",
    "                if augument>0.5:\n",
    "                    content = self.dropout(content,p=0.3)\n",
    "                else:\n",
    "                    content = self.shuffle(content)\n",
    "\n",
    "            data =t.from_numpy(content).long()\n",
    "            label_dict = {label:t.from_numpy(self.Y[label][index]).long() for label in self.label_list}\n",
    "            return data, label_dict\n",
    "        else:\n",
    "            return t.from_numpy(content).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict    \n",
    "def val(model,dataset,val_opt:FGSentimetDataset):\n",
    "    '''\n",
    "    计算模型在验证集上的分数\n",
    "    '''\n",
    "    opt = val_opt\n",
    "    model.eval()\n",
    "    dataloader = data.DataLoader(dataset,\n",
    "                    batch_size = opt.batch_size,\n",
    "                    shuffle = False,\n",
    "                    num_workers = opt.num_workers,\n",
    "                    pin_memory = True\n",
    "                    )\n",
    "    \n",
    "    predict_dict = defaultdict(list)\n",
    "    for ii, content in tqdm.tqdm(enumerate(dataloader)):\n",
    "        content =  content.cuda()\n",
    "        predict_dict = model(content)\n",
    "        for col_name in model.label_list:\n",
    "            predict_dict[col_name].extend(predict_dict[col_name].cpu().tolist())\n",
    "    scores = get_score(dataset.Y, predict_dict)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "dataloader = torch.utils.data.DataLoader(train,\n",
    "                batch_size = 256,\n",
    "                shuffle = True,\n",
    "                num_workers = 8,\n",
    "                pin_memory = True\n",
    "                )\n",
    "optimizer = model.get_optimizer(5e-3, 1e-3, 0.99)\n",
    "best_score = 0\n",
    "loss_function = torch.nn.MultiLabelSoftMarginLoss()\n",
    "model.cuda()\n",
    "for epoch in range(100):\n",
    "    for ii,(content,label) in tqdm.tqdm(enumerate(dataloader)):\n",
    "        # 训练 更新参数\n",
    "        content,label = content.long().cuda(),label.long().cuda()\n",
    "        optimizer.zero_grad()\n",
    "        score = model(content)\n",
    "        loss = loss_function(score,label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
