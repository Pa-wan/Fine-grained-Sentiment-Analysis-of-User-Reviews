{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch as t\n",
    "import time\n",
    "\n",
    "class BasicModule(t.nn.Module):\n",
    "    '''\n",
    "    封装了nn.Module,主要是提供了save和load两个方法\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicModule,self).__init__()\n",
    "        self.model_name=str(type(self))# 默认名字\n",
    "\n",
    "    def load(self, path,change_opt=True):\n",
    "        print(\"Loading model from \" + path)\n",
    "        data = t.load(path)\n",
    "        self.load_state_dict(data)\n",
    "        return self.cuda()\n",
    "\n",
    "    def save(self, name=None,new=False):\n",
    "        prefix = '../ckpt/' + self.model_name + '_'\n",
    "        if name is None:\n",
    "            name = time.strftime('%m%d_%H:%M:%S.pth')\n",
    "        path = prefix+name +'.pt'\n",
    "        data=self.state_dict()\n",
    "        t.save(data, path)\n",
    "        print(\"Saving model to \"+ path)\n",
    "        return path\n",
    "\n",
    "    def get_optimizer(self,lr1,lr2=0,weight_decay = 0):\n",
    "        ignored_params = list(map(id, self.encoder.parameters()))\n",
    "        base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "                        self.parameters())\n",
    "        if lr2 is None: lr2 = lr1*0.5 \n",
    "        optimizer = t.optim.Adam([\n",
    "                dict(params=base_params,weight_decay = weight_decay,lr=lr1),\n",
    "                {'params': self.encoder.parameters(), 'lr': lr2}\n",
    "            ])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self,cin,co,relu=True,norm=True):\n",
    "        super(Inception, self).__init__()\n",
    "        assert(co%4==0)\n",
    "        cos=[co//4]*4\n",
    "        self.activa=nn.Sequential()\n",
    "        if norm:self.activa.add_module('norm',nn.BatchNorm1d(co))\n",
    "        if relu:self.activa.add_module('relu',nn.ReLU(True))\n",
    "        self.branch1 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[0], 1,stride=1)),\n",
    "            ])) \n",
    "        self.branch2 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[1], 1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[1])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "        self.branch3 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[2], 3,padding=1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[2])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n",
    "            ]))\n",
    "        self.branch4 =nn.Sequential(OrderedDict([\n",
    "            #('pool',nn.MaxPool1d(2)),\n",
    "            ('conv3', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "    def forward(self,x):\n",
    "        branch1=self.branch1(x)\n",
    "        branch2=self.branch2(x)\n",
    "        branch3=self.branch3(x)\n",
    "        branch4=self.branch4(x)\n",
    "        result=self.activa(torch.cat((branch1,branch2,branch3,branch4),1))\n",
    "        return result\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle as pkl\n",
    "class CNNText_inception(BasicModule):\n",
    "    def __init__(self, opt, model_pre=''):\n",
    "        super(CNNText_inception, self).__init__()\n",
    "        incept_dim=opt.inception_dim\n",
    "        self.model_name = 'CNNText_inception' + model_pre\n",
    "        self.opt=opt\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "        self.content_conv=nn.Sequential(\n",
    "            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)\n",
    "            #Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)\n",
    "            Inception(incept_dim,incept_dim),\n",
    "            nn.MaxPool1d(opt.content_seq_len)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(incept_dim,opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n",
    "        )\n",
    "        if opt.embedding_path:\n",
    "            print('load embedding')\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        content_out=self.content_conv(content.permute(0,2,1))        \n",
    "        out = content_out.view(content_out.size(0), -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class RCNN(BasicModule): \n",
    "    def __init__(self, opt, model_pre=''):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.model_name = 'RCNN' + model_pre\n",
    "        self.opt=opt\n",
    "        kernel_size = opt.kernel_size\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "\n",
    "        self.title_lstm = nn.LSTM(input_size = opt.embedding_dim,\\\n",
    "                            hidden_size = opt.hidden_size,\n",
    "                            num_layers = opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            dropout = 0.5,\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "        self.content_lstm =nn.LSTM(input_size = opt.embedding_dim,\\\n",
    "                            hidden_size = opt.hidden_size,\n",
    "                            num_layers = opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            # dropout = 0.5,\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "        self.content_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = opt.hidden_size*2 + opt.embedding_dim,\n",
    "                      out_channels = opt.content_dim,\n",
    "                      kernel_size =  kernel_size),\n",
    "            nn.BatchNorm1d(opt.content_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(in_channels = opt.content_dim,\n",
    "                      out_channels = opt.content_dim,\n",
    "                      kernel_size =  kernel_size),\n",
    "            nn.BatchNorm1d(opt.content_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(opt.kmax_pooling*(opt.content_dim),opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n",
    "        )\n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "\n",
    "        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n",
    "        content_em = (content).permute(0,2,1)\n",
    "        content_out = t.cat((content_out,content_em),dim=1)\n",
    "        content_conv_out = kmax_pooling(self.content_conv(content_out),2,self.opt.kmax_pooling)\n",
    "        out = content_conv_out.view(content_conv_out.size(0), -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf8\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "class ModelConfig(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "    content_dim = 200 #描述的卷积核数\n",
    "    num_classes = 4 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1024 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2# k\n",
    "    hidden_size = 256 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 512 #inception的卷积核数\n",
    "    \n",
    "    vocab_size = 65462\n",
    "    kernel_size = 3 #单尺度卷积核\n",
    "    kernel_sizes = [2,3,4] #多尺度卷积核\n",
    "    content_seq_len = 200 \n",
    "    embedding_path = '../inputs/fasttextwordvec.npz' # Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelopt = ModelConfig()\n",
    "modelopt = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, Y_train_f = \"../inputs/X_train.npz\",\"../inputs/Y_train.npz\"\n",
    "X_valid_f, Y_valid_f = \"../inputs/X_valid.npz\",\"../inputs/Y_valid.npz\"\n",
    "X_test_f = \"../inputs/X_test.npz\"\n",
    "X_train_tensor = t.from_numpy(np.load(X_train_f)['X']).long()\n",
    "X_valid_tensor = t.from_numpy(np.load(X_valid_f)['X']).long()\n",
    "X_test_tensor = t.from_numpy(np.load(X_test_f)['X']).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_dict = np.load(Y_train_f)\n",
    "Y_valid_dict = np.load(Y_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "columns = pkl.load(open(\"../inputs/columns.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, valiloader,y_true, use_cuda=True):\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for bx in validloader:\n",
    "        if use_cuda:\n",
    "            bx =  bx.cuda()\n",
    "        y_pre = model(bx)\n",
    "        y_label = torch.max(y_pre, 1)[1].data\n",
    "        y_pred.extend(y_label.tolist())\n",
    "    y_true_ = np.argmax(y_true, 1)\n",
    "    f1 =  metrics.f1_score(y_true_, y_pred, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(model, testloader, use_cuda=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for bx in testloader:\n",
    "        if use_cuda:\n",
    "            bx = bx.cuda()\n",
    "        y_pre = model(bx)\n",
    "        y_label = torch.max(y_pre, 1)[1].data\n",
    "        y_pred.extend(y_label.tolist())\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSentimetDataset(data.Dataset):\n",
    "    def __init__(self, X_npz, Y_npz, label_pkl, augument=False, training=False, dropout_rate=0.3, augument_rate=0.4):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.augument_rate = augument_rate\n",
    "        self.augument=augument\n",
    "        self.training = training\n",
    "        self.label_list = pickle.load(open(label_pkl, 'rb'))\n",
    "        self.X = np.load(X_npz)['X']\n",
    "        dataset = np.load(Y_npz)\n",
    "        self.Y = {}\n",
    "        for col in self.label_list:\n",
    "            self.Y[col] = dataset[col]\n",
    "        self._len = self.X.shape[0]\n",
    "    def shuffle(self,d):\n",
    "        return np.random.permutation(d.tolist())\n",
    "\n",
    "    def dropout(self,d,p=0.5):\n",
    "        len_ = len(d)\n",
    "        index = np.random.choice(len_,int(len_*p))\n",
    "        d[index]=0\n",
    "        return d     \n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        content =  self.X[index]\n",
    "    \n",
    "        if self.training:  \n",
    "            if self.augument :\n",
    "                augument=random.random()\n",
    "\n",
    "                if augument>self.augument_rate:\n",
    "                    content = self.dropout(content,p=self.dropout_rate)\n",
    "                else:\n",
    "                    content = self.shuffle(content)\n",
    "\n",
    "            data =t.from_numpy(content).long()\n",
    "            label_dict = {label:t.from_numpy(self.Y[label][index]).long() for label in self.label_list}\n",
    "            return data, label_dict\n",
    "        else:\n",
    "            return t.from_numpy(content).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for col: location_traffic_convenience\n",
      "load embedding\n",
      "training epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/xq/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 114, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/xq/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 200 and 4 in dimension 2 at /home/xq/packages/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7c02f69c5bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"training epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/xq/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 114, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/xq/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 200 and 4 in dimension 2 at /home/xq/packages/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "Valid_X = torch.utils.data.DataLoader(X_valid_tensor,\n",
    "                            batch_size = 128,\n",
    "                            shuffle = False,\n",
    "                            num_workers = 12,\n",
    "                            pin_memory = True)\n",
    "Test_X = torch.utils.data.DataLoader(X_test_tensor,\n",
    "                            batch_size = 128,\n",
    "                            shuffle = False,\n",
    "                            num_workers = 12,\n",
    "                            pin_memory = True)\n",
    "Test_pre_dict = {}\n",
    "for col in columns:\n",
    "    print(f\"Training model for col: {col}\")\n",
    "    Y_train_tensor = t.from_numpy(Y_train_dict[col]).long()\n",
    "    model =  CNNText_inception(ModelConfig, col)\n",
    "    Train_loader = torch.utils.data.DataLoader(TrainDataSet(X_train_tensor, Y_train_tensor),\n",
    "                                batch_size = 128,\n",
    "                                shuffle = True,\n",
    "                                num_workers = 12,\n",
    "                                pin_memory = True)\n",
    "    Valid_Y = Y_valid_dict[col]\n",
    "    best_score = 0\n",
    "    lr = 5e-3\n",
    "    lr2 = 1e-3\n",
    "    lr_decay = 0.9\n",
    "    i = 0\n",
    "    optimizer = model.get_optimizer(lr, lr2,lr_decay)\n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        print(f\"training epoch {epoch}\")\n",
    "        for ii,(content, true) in enumerate(Train_loader):\n",
    "            content = content.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            pred_dict = model(content)\n",
    "            loss = loss_function(pred, torch.max(true.cuda(), 1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scores = eval(model,Valid_X, Valid_Y)\n",
    "        print(f\"epoch: {epoch} LR: {lr}, F1_score:{scores}\")\n",
    "        if scores>best_score:\n",
    "            print(f\"F1-score improved from {best_score} to {scores}\")\n",
    "            i = 0\n",
    "            best_score = scores\n",
    "            best_path = model.save(name = str(scores),new=True)\n",
    "        if scores < best_score:\n",
    "            i += 1\n",
    "            print(f\"F1-score did not improved from {best_score} for {i} epochs\")       \n",
    "            model.load(best_path,change_opt=False)\n",
    "            lr = lr * lr_decay\n",
    "            lr2= 2e-4 if lr2==0 else  lr2*0.8\n",
    "            optimizer = model.get_optimizer(lr,lr2,0)          \n",
    "            \n",
    "            \n",
    "    Test_pre_dict[col] =  pred_test(model, Test_X, use_cuda=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_metrics(y_pred_dict, y_true_dict):\n",
    "    accuracys = {}\n",
    "    f1s = {}\n",
    "    for col, y_pred in y_pred_dict.items():\n",
    "        # accuracys[col] = metrics.accuracy_score(y_true_dict[col], y_pred)\n",
    "        f1s[col] = metrics.f1_score(y_true_dict[col], y_pred, average='macro')\n",
    "\n",
    "    return f1s\n",
    "\n",
    "def eval(model, valiloader,y_true, use_cuda=True):\n",
    "    \n",
    "    model.eval()\n",
    "    y_true_ = {}\n",
    "    y_pred = defaultdict(list)\n",
    "    for bx in validloader:\n",
    "        if use_cuda:\n",
    "            bx =  bx.cuda()\n",
    "        y_pre = model(bx)\n",
    "        for col in model.label_list:\n",
    "            y_label = torch.max(y_pre[col], 1)[1].data\n",
    "            y_pred[col].extend(y_label.tolist())\n",
    "            y_true_[col] = np.argmax(y_true[col], 1)\n",
    "    f1s = eval_metrics(y_pred, y_true_)\n",
    "    print(json.dumps(f1s, indent=4, sort_keys=True))\n",
    "    return np.mean(list(f1s.values()))\n",
    "\n",
    "#scores = eval(model,validloader, Valid_dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "best_score = 0\n",
    "lr = 5e-3\n",
    "lr2 = 1e-3\n",
    "lr_decay = 0.9\n",
    "early_stops = 20\n",
    "i = 0\n",
    "optimizer = model.get_optimizer(lr, lr2,lr_decay)\n",
    "model.cuda()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    print(f\"training epoch {epoch}\")\n",
    "    for ii,(content, true_dict) in enumerate(trainloader):\n",
    "        content = content.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred_dict = model(content)\n",
    "        loss = torch.mean(torch.stack([loss_function(pred, torch.max(true_dict[col].cuda(), 1)[1]) for col, pred in pred_dict.items()]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scores = eval(model,validloader, Valid_dataset.Y)\n",
    "    print(f\"epoch: {epoch} LR: {lr}, F1_score:{scores}\")\n",
    "    if scores>best_score:\n",
    "        print(f\"F1-score improved from {best_score} to {scores}\")\n",
    "        i = 0\n",
    "        best_score = scores\n",
    "        best_path = model.save(name = str(scores),new=True)\n",
    "\n",
    "    if scores < best_score:\n",
    "        i += 1\n",
    "        print(f\"F1-score did not improved from {best_score} for {i} epochs\")       \n",
    "        model.load(best_path,change_opt=False)\n",
    "        if i > early_stops:\n",
    "            print(\"Stop training\")\n",
    "            break\n",
    "        lr = lr * lr_decay\n",
    "        lr2= 2e-4 if lr2==0 else  lr2*0.8\n",
    "        optimizer = model.get_optimizer(lr,lr2,0)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.load(\"../inputs/X_test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(torch.from_numpy(test_arr['X']).long(), batch_size=1024, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(model, testloader, use_cuda=True):\n",
    "    model.eval()\n",
    "    y_pred = defaultdict(list)\n",
    "    for bx in testloader:\n",
    "        if use_cuda:\n",
    "            bx = bx.cuda()\n",
    "        y_pre = model(bx)\n",
    "        for col in model.label_list:\n",
    "            y_label = torch.max(y_pre[col], 1)[1].data\n",
    "            y_pred[col].extend(y_label.tolist())\n",
    "    return y_pred\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pred_test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data_from_csv(file_name, header=0, encoding=\"utf-8\"):\n",
    "\n",
    "    data_df = pd.read_csv(file_name, header=header, encoding=encoding)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data_from_csv(\"../inputs/sentiment_analysis_testa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, pred in pred_test.items():\n",
    "    test[col] = pred\n",
    "    test[col] -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"../output/RCNN1.csv\", encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils.data.dataloader' from '/home/xq/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
